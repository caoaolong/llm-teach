{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| 算法                          | 代表模型/应用                    | 基本原理                                                    | 优点                                                  | 缺点                                      |\n",
    "|-----------------------------|----------------------------|---------------------------------------------------------|-----------------------------------------------------|-----------------------------------------|\n",
    "| **BPE（Byte Pair Encoding）** | GPT-2、RoBERTa、Fairseq MT   | 从字符出发，统计训练语料中频繁出现的字符对（pair），迭代合并为更大的子词单元，直到达到设定词表大小。    | - 简单高效，易实现<br>- 兼顾字与词的粒度<br>- OOV 问题显著缓解            | - 切分结果不唯一，依赖语料<br>- 可能将常见词切得很碎，影响语义一致性  |\n",
    "| **WordPiece**               | BERT、ALBERT、DistilBERT     | 与 BPE 类似，但基于**概率最大化**：选择能最大提升训练语料似然的子词单元，而非仅按频率合并。      | - 更注重语言建模效果<br>- 与 MLM（掩码语言模型）自然匹配<br>- 在大规模语料上更稳健  | - 算法复杂度高于 BPE<br>- 子词切分结果较难解释           |\n",
    "| **Unigram Language Model**  | SentencePiece（默认）、XLNet、T5 | 假设一个固定大小的子词表，定义每个子词的概率；通过 EM 算法迭代优化，保留高概率子词，舍弃低概率子词。    | - 全局优化，不依赖贪心合并<br>- 子词粒度更平衡<br>- 可提供多个切分候选，利于采样     | - 训练过程更复杂<br>- 对低资源语料可能过拟合              |\n",
    "| **SentencePiece 工具**        | T5、mT5、XLM-R、ChatGLM       | 工具层实现：可训练 **BPE 或 Unigram**，直接在原始文本或字节级别处理，无需预分词，支持多语言。 | - 语言无关，统一流程<br>- 支持直接基于原始文本/字节<br>- 训练与推理效率高，已成工业标准 | - 不解决模型层面的长序列开销<br>- 切分仍依赖语料分布，跨领域时可能不稳 |\n",
    "\n",
    "---\n",
    "\n",
    "📌 **脉络总结**：\n",
    "\n",
    "* **BPE**：频率驱动 → 简单实用\n",
    "* **WordPiece**：似然驱动 → 更适合预训练模型\n",
    "* **Unigram LM**：概率建模 → 更灵活、全局最优\n",
    "* **SentencePiece**：工程化工具 → 标准实现，多语言友好\n"
   ],
   "id": "cb477757dbc37fdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "假设词库\n",
    "\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "\n",
    "\n",
    "假设词频\n",
    "\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "\n",
    "\n",
    "拆分\n",
    "\n",
    "(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
    "\n",
    "\n",
    "统计Pair频率\n",
    "\n",
    "(\"u\" \"g\", 20), ...\n",
    "\n",
    "\n",
    "高频合并\n",
    "\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
    "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "\n",
    "\n",
    "统计Pair频率\n",
    "\n",
    "(\"u\" \"n\", 16), ...\n",
    "\n",
    "\n",
    "高频合并\n",
    "\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
    "Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "\n",
    "\n",
    "统计Pair频率\n",
    "\n",
    "(\"h\" \"ug\", 15), ...\n",
    "\n",
    "\n",
    "高频合并\n",
    "\n",
    "Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n",
    "```"
   ],
   "id": "cdd6c7cb0f1cc9b2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T10:46:02.565734Z",
     "start_time": "2025-08-27T10:46:02.561619Z"
    }
   },
   "source": [
    "text = '''\n",
    "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:46:05.882579Z",
     "start_time": "2025-08-27T10:46:05.878082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "words = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "corpus = [\" \".join(list(w)) + \" </w>\" for w in words]\n",
    "print(corpus)"
   ],
   "id": "87f6ba4eb066b9d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W e </w>', 'i n t r o d u c e </w>', 'o u r </w>', 'f i r s t </w>', '- </w>', 'g e n e r a t i o n </w>', 'r e a s o n i n g </w>', 'm o d e l s </w>', ', </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'Z e r o </w>', 'a n d </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '. </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'Z e r o </w>', ', </w>', 'a </w>', 'm o d e l </w>', 't r a i n e d </w>', 'v i a </w>', 'l a r g e </w>', '- </w>', 's c a l e </w>', 'r e i n f o r c e m e n t </w>', 'l e a r n i n g </w>', '( </w>', 'R L </w>', ') </w>', 'w i t h o u t </w>', 's u p e r v i s e d </w>', 'f i n e </w>', '- </w>', 't u n i n g </w>', '( </w>', 'S F T </w>', ') </w>', 'a s </w>', 'a </w>', 'p r e l i m i n a r y </w>', 's t e p </w>', ', </w>', 'd e m o n s t r a t e d </w>', 'r e m a r k a b l e </w>', 'p e r f o r m a n c e </w>', 'o n </w>', 'r e a s o n i n g </w>', '. </w>', 'W i t h </w>', 'R L </w>', ', </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'Z e r o </w>', 'n a t u r a l l y </w>', 'e m e r g e d </w>', 'w i t h </w>', 'n u m e r o u s </w>', 'p o w e r f u l </w>', 'a n d </w>', 'i n t e r e s t i n g </w>', 'r e a s o n i n g </w>', 'b e h a v i o r s </w>', '. </w>', 'H o w e v e r </w>', ', </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'Z e r o </w>', 'e n c o u n t e r s </w>', 'c h a l l e n g e s </w>', 's u c h </w>', 'a s </w>', 'e n d l e s s </w>', 'r e p e t i t i o n </w>', ', </w>', 'p o o r </w>', 'r e a d a b i l i t y </w>', ', </w>', 'a n d </w>', 'l a n g u a g e </w>', 'm i x i n g </w>', '. </w>', 'T o </w>', 'a d d r e s s </w>', 't h e s e </w>', 'i s s u e s </w>', 'a n d </w>', 'f u r t h e r </w>', 'e n h a n c e </w>', 'r e a s o n i n g </w>', 'p e r f o r m a n c e </w>', ', </w>', 'w e </w>', 'i n t r o d u c e </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', ', </w>', 'w h i c h </w>', 'i n c o r p o r a t e s </w>', 'c o l d </w>', '- </w>', 's t a r t </w>', 'd a t a </w>', 'b e f o r e </w>', 'R L </w>', '. </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', 'a c h i e v e s </w>', 'p e r f o r m a n c e </w>', 'c o m p a r a b l e </w>', 't o </w>', 'O p e n A I </w>', '- </w>', 'o 1 </w>', 'a c r o s s </w>', 'm a t h </w>', ', </w>', 'c o d e </w>', ', </w>', 'a n d </w>', 'r e a s o n i n g </w>', 't a s k s </w>', '. </w>', 'T o </w>', 's u p p o r t </w>', 't h e </w>', 'r e s e a r c h </w>', 'c o m m u n i t y </w>', ', </w>', 'w e </w>', 'h a v e </w>', 'o p e n </w>', '- </w>', 's o u r c e d </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'Z e r o </w>', ', </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', ', </w>', 'a n d </w>', 's i x </w>', 'd e n s e </w>', 'm o d e l s </w>', 'd i s t i l l e d </w>', 'f r o m </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', 'b a s e d </w>', 'o n </w>', 'L l a m a </w>', 'a n d </w>', 'Q w e n </w>', '. </w>', 'D e e p S e e k </w>', '- </w>', 'R 1 </w>', '- </w>', 'D i s t i l l </w>', '- </w>', 'Q w e n </w>', '- </w>', '3 2 B </w>', 'o u t p e r f o r m s </w>', 'O p e n A I </w>', '- </w>', 'o 1 </w>', '- </w>', 'm i n i </w>', 'a c r o s s </w>', 'v a r i o u s </w>', 'b e n c h m a r k s </w>', ', </w>', 'a c h i e v i n g </w>', 'n e w </w>', 's t a t e </w>', '- </w>', 'o f </w>', '- </w>', 't h e </w>', '- </w>', 'a r t </w>', 'r e s u l t s </w>', 'f o r </w>', 'd e n s e </w>', 'm o d e l s </w>', '. </w>']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:46:09.960930Z",
     "start_time": "2025-08-27T10:46:09.956056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "word_freqs = Counter(corpus)\n",
    "print(type(word_freqs.items()))\n",
    "print(word_freqs)"
   ],
   "id": "7c5e40f4b6cae253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_items'>\n",
      "Counter({'- </w>': 30, ', </w>': 15, 'D e e p S e e k </w>': 11, 'R 1 </w>': 11, '. </w>': 8, 'a n d </w>': 7, 'r e a s o n i n g </w>': 5, 'Z e r o </w>': 5, 'm o d e l s </w>': 3, 'R L </w>': 3, 'p e r f o r m a n c e </w>': 3, 'i n t r o d u c e </w>': 2, 'a </w>': 2, '( </w>': 2, ') </w>': 2, 'a s </w>': 2, 'o n </w>': 2, 'T o </w>': 2, 'w e </w>': 2, 'O p e n A I </w>': 2, 'o 1 </w>': 2, 'a c r o s s </w>': 2, 't h e </w>': 2, 'd e n s e </w>': 2, 'Q w e n </w>': 2, 'W e </w>': 1, 'o u r </w>': 1, 'f i r s t </w>': 1, 'g e n e r a t i o n </w>': 1, 'm o d e l </w>': 1, 't r a i n e d </w>': 1, 'v i a </w>': 1, 'l a r g e </w>': 1, 's c a l e </w>': 1, 'r e i n f o r c e m e n t </w>': 1, 'l e a r n i n g </w>': 1, 'w i t h o u t </w>': 1, 's u p e r v i s e d </w>': 1, 'f i n e </w>': 1, 't u n i n g </w>': 1, 'S F T </w>': 1, 'p r e l i m i n a r y </w>': 1, 's t e p </w>': 1, 'd e m o n s t r a t e d </w>': 1, 'r e m a r k a b l e </w>': 1, 'W i t h </w>': 1, 'n a t u r a l l y </w>': 1, 'e m e r g e d </w>': 1, 'w i t h </w>': 1, 'n u m e r o u s </w>': 1, 'p o w e r f u l </w>': 1, 'i n t e r e s t i n g </w>': 1, 'b e h a v i o r s </w>': 1, 'H o w e v e r </w>': 1, 'e n c o u n t e r s </w>': 1, 'c h a l l e n g e s </w>': 1, 's u c h </w>': 1, 'e n d l e s s </w>': 1, 'r e p e t i t i o n </w>': 1, 'p o o r </w>': 1, 'r e a d a b i l i t y </w>': 1, 'l a n g u a g e </w>': 1, 'm i x i n g </w>': 1, 'a d d r e s s </w>': 1, 't h e s e </w>': 1, 'i s s u e s </w>': 1, 'f u r t h e r </w>': 1, 'e n h a n c e </w>': 1, 'w h i c h </w>': 1, 'i n c o r p o r a t e s </w>': 1, 'c o l d </w>': 1, 's t a r t </w>': 1, 'd a t a </w>': 1, 'b e f o r e </w>': 1, 'a c h i e v e s </w>': 1, 'c o m p a r a b l e </w>': 1, 't o </w>': 1, 'm a t h </w>': 1, 'c o d e </w>': 1, 't a s k s </w>': 1, 's u p p o r t </w>': 1, 'r e s e a r c h </w>': 1, 'c o m m u n i t y </w>': 1, 'h a v e </w>': 1, 'o p e n </w>': 1, 's o u r c e d </w>': 1, 's i x </w>': 1, 'd i s t i l l e d </w>': 1, 'f r o m </w>': 1, 'b a s e d </w>': 1, 'L l a m a </w>': 1, 'D i s t i l l </w>': 1, '3 2 B </w>': 1, 'o u t p e r f o r m s </w>': 1, 'm i n i </w>': 1, 'v a r i o u s </w>': 1, 'b e n c h m a r k s </w>': 1, 'a c h i e v i n g </w>': 1, 'n e w </w>': 1, 's t a t e </w>': 1, 'o f </w>': 1, 'a r t </w>': 1, 'r e s u l t s </w>': 1, 'f o r </w>': 1})\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:46:13.831819Z",
     "start_time": "2025-08-27T10:46:13.827395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(_word_freqs):\n",
    "    \"\"\"统计所有bigram出现频率\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for _word, _freq in _word_freqs.items():\n",
    "        symbols = _word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += _freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"执行一次合并\"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    for _word in v_in:\n",
    "        w_out = p.sub(\"\".join(pair), _word)\n",
    "        v_out[w_out] = v_in[_word]\n",
    "    return v_out"
   ],
   "id": "87095a1deea83785",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:46:18.089458Z",
     "start_time": "2025-08-27T10:46:18.083548Z"
    }
   },
   "cell_type": "code",
   "source": "print(get_stats(word_freqs))",
   "id": "eda0a7d1f90e8601",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('W', 'e'): 1, ('e', '</w>'): 24, ('i', 'n'): 19, ('n', 't'): 5, ('t', 'r'): 4, ('r', 'o'): 11, ('o', 'd'): 7, ('d', 'u'): 2, ('u', 'c'): 3, ('c', 'e'): 8, ('o', 'u'): 7, ('u', 'r'): 4, ('r', '</w>'): 5, ('f', 'i'): 2, ('i', 'r'): 1, ('r', 's'): 3, ('s', 't'): 8, ('t', '</w>'): 6, ('-', '</w>'): 30, ('g', 'e'): 5, ('e', 'n'): 14, ('n', 'e'): 4, ('e', 'r'): 18, ('r', 'a'): 6, ('a', 't'): 7, ('t', 'i'): 6, ('i', 'o'): 4, ('o', 'n'): 10, ('n', '</w>'): 7, ('r', 'e'): 15, ('e', 'a'): 8, ('a', 's'): 9, ('s', 'o'): 6, ('n', 'i'): 9, ('n', 'g'): 12, ('g', '</w>'): 10, ('m', 'o'): 5, ('d', 'e'): 8, ('e', 'l'): 5, ('l', 's'): 3, ('s', '</w>'): 21, (',', '</w>'): 15, ('D', 'e'): 11, ('e', 'e'): 22, ('e', 'p'): 13, ('p', 'S'): 11, ('S', 'e'): 11, ('e', 'k'): 11, ('k', '</w>'): 11, ('R', '1'): 11, ('1', '</w>'): 13, ('Z', 'e'): 5, ('o', '</w>'): 8, ('a', 'n'): 12, ('n', 'd'): 8, ('d', '</w>'): 15, ('.', '</w>'): 8, ('a', '</w>'): 5, ('l', '</w>'): 3, ('a', 'i'): 1, ('e', 'd'): 7, ('v', 'i'): 4, ('i', 'a'): 1, ('l', 'a'): 3, ('a', 'r'): 10, ('r', 'g'): 2, ('s', 'c'): 1, ('c', 'a'): 1, ('a', 'l'): 3, ('l', 'e'): 7, ('e', 'i'): 1, ('n', 'f'): 1, ('f', 'o'): 7, ('o', 'r'): 12, ('r', 'c'): 3, ('e', 'm'): 4, ('m', 'e'): 3, ('r', 'n'): 1, ('(', '</w>'): 2, ('R', 'L'): 3, ('L', '</w>'): 3, (')', '</w>'): 2, ('w', 'i'): 2, ('i', 't'): 6, ('t', 'h'): 8, ('h', 'o'): 1, ('u', 't'): 2, ('s', 'u'): 5, ('u', 'p'): 2, ('p', 'e'): 9, ('r', 'v'): 1, ('i', 's'): 4, ('s', 'e'): 6, ('t', 'u'): 2, ('u', 'n'): 3, ('S', 'F'): 1, ('F', 'T'): 1, ('T', '</w>'): 1, ('p', 'r'): 1, ('l', 'i'): 2, ('i', 'm'): 1, ('m', 'i'): 3, ('n', 'a'): 2, ('r', 'y'): 1, ('y', '</w>'): 4, ('t', 'e'): 6, ('p', '</w>'): 1, ('n', 's'): 3, ('m', 'a'): 7, ('r', 'k'): 2, ('k', 'a'): 1, ('a', 'b'): 3, ('b', 'l'): 2, ('r', 'f'): 5, ('r', 'm'): 4, ('n', 'c'): 7, ('W', 'i'): 1, ('h', '</w>'): 6, ('l', 'l'): 4, ('l', 'y'): 1, ('n', 'u'): 1, ('u', 'm'): 1, ('u', 's'): 2, ('p', 'o'): 4, ('o', 'w'): 2, ('w', 'e'): 6, ('f', 'u'): 2, ('u', 'l'): 2, ('e', 's'): 10, ('b', 'e'): 3, ('e', 'h'): 1, ('h', 'a'): 4, ('a', 'v'): 2, ('H', 'o'): 1, ('e', 'v'): 3, ('v', 'e'): 3, ('c', 'o'): 6, ('c', 'h'): 7, ('d', 'l'): 1, ('s', 's'): 5, ('e', 't'): 1, ('o', 'o'): 1, ('a', 'd'): 2, ('d', 'a'): 2, ('b', 'i'): 1, ('i', 'l'): 3, ('t', 'y'): 2, ('g', 'u'): 1, ('u', 'a'): 1, ('a', 'g'): 1, ('i', 'x'): 2, ('x', 'i'): 1, ('T', 'o'): 2, ('d', 'd'): 1, ('d', 'r'): 1, ('h', 'e'): 4, ('u', 'e'): 1, ('r', 't'): 4, ('n', 'h'): 1, ('w', 'h'): 1, ('h', 'i'): 3, ('i', 'c'): 1, ('r', 'p'): 1, ('o', 'l'): 1, ('l', 'd'): 1, ('t', 'a'): 4, ('e', 'f'): 1, ('a', 'c'): 4, ('i', 'e'): 2, ('o', 'm'): 3, ('m', 'p'): 1, ('p', 'a'): 1, ('t', 'o'): 1, ('O', 'p'): 2, ('n', 'A'): 2, ('A', 'I'): 2, ('I', '</w>'): 2, ('o', '1'): 2, ('c', 'r'): 2, ('o', 's'): 2, ('s', 'k'): 1, ('k', 's'): 2, ('p', 'p'): 1, ('m', 'm'): 1, ('m', 'u'): 1, ('o', 'p'): 1, ('s', 'i'): 1, ('x', '</w>'): 1, ('d', 'i'): 1, ('f', 'r'): 1, ('m', '</w>'): 1, ('b', 'a'): 1, ('L', 'l'): 1, ('a', 'm'): 1, ('Q', 'w'): 2, ('D', 'i'): 1, ('3', '2'): 1, ('2', 'B'): 1, ('B', '</w>'): 1, ('t', 'p'): 1, ('m', 's'): 1, ('i', '</w>'): 1, ('v', 'a'): 1, ('r', 'i'): 1, ('h', 'm'): 1, ('e', 'w'): 1, ('w', '</w>'): 1, ('o', 'f'): 1, ('f', '</w>'): 1, ('l', 't'): 1, ('t', 's'): 1})\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:09:22.160531Z",
     "start_time": "2025-08-27T11:09:22.141430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_merges = 50  # 执行的合并次数\n",
    "vocab = word_freqs.copy()\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)  # 选择出现次数最多的bigram\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f\"Step {i+1}: merge {best}\")\n",
    "\n",
    "bpe_vocab = set()\n",
    "for word in vocab:\n",
    "    for symbol in word.split():\n",
    "        bpe_vocab.add(symbol)\n",
    "\n",
    "print(\"最终BPE词表:\")\n",
    "print(sorted(bpe_vocab))"
   ],
   "id": "1a372237e7901ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: merge ('-', '</w>')\n",
      "Step 2: merge ('e', '</w>')\n",
      "Step 3: merge ('e', 'e')\n",
      "Step 4: merge ('s', '</w>')\n",
      "Step 5: merge ('i', 'n')\n",
      "Step 6: merge ('e', 'r')\n",
      "Step 7: merge (',', '</w>')\n",
      "Step 8: merge ('d', '</w>')\n",
      "Step 9: merge ('e', 'n')\n",
      "Step 10: merge ('r', 'e')\n",
      "Step 11: merge ('1', '</w>')\n",
      "Step 12: merge ('a', 'n')\n",
      "Step 13: merge ('o', 'r')\n",
      "Step 14: merge ('D', 'ee')\n",
      "Step 15: merge ('Dee', 'p')\n",
      "Step 16: merge ('Deep', 'S')\n",
      "Step 17: merge ('DeepS', 'ee')\n",
      "Step 18: merge ('DeepSee', 'k')\n",
      "Step 19: merge ('DeepSeek', '</w>')\n",
      "Step 20: merge ('R', '1</w>')\n",
      "Step 21: merge ('o', 'n')\n",
      "Step 22: merge ('in', 'g')\n",
      "Step 23: merge ('ing', '</w>')\n",
      "Step 24: merge ('a', 'r')\n",
      "Step 25: merge ('s', 't')\n",
      "Step 26: merge ('o', '</w>')\n",
      "Step 27: merge ('.', '</w>')\n",
      "Step 28: merge ('t', 'h')\n",
      "Step 29: merge ('o', 'd')\n",
      "Step 30: merge ('o', 'u')\n",
      "Step 31: merge ('a', 's')\n",
      "Step 32: merge ('an', 'd</w>')\n",
      "Step 33: merge ('e', 'd</w>')\n",
      "Step 34: merge ('f', 'or')\n",
      "Step 35: merge ('c', 'h')\n",
      "Step 36: merge ('c', 'e</w>')\n",
      "Step 37: merge ('a', 't')\n",
      "Step 38: merge ('re', 'as')\n",
      "Step 39: merge ('reas', 'on')\n",
      "Step 40: merge ('reason', 'ing</w>')\n",
      "Step 41: merge ('Z', 'er')\n",
      "Step 42: merge ('Zer', 'o</w>')\n",
      "Step 43: merge ('a', '</w>')\n",
      "Step 44: merge ('t', '</w>')\n",
      "Step 45: merge ('s', 'u')\n",
      "Step 46: merge ('p', 'er')\n",
      "Step 47: merge ('on', '</w>')\n",
      "Step 48: merge ('m', 'od')\n",
      "Step 49: merge ('mod', 'e')\n",
      "Step 50: merge ('mode', 'l')\n",
      "最终BPE词表:\n",
      "['(', ')', ',</w>', '-</w>', '.</w>', '1</w>', '2', '3', '</w>', 'A', 'B', 'D', 'DeepSeek</w>', 'F', 'H', 'I', 'L', 'O', 'Q', 'R', 'R1</w>', 'S', 'T', 'W', 'Zero</w>', 'a', 'a</w>', 'an', 'and</w>', 'ar', 'as', 'at', 'b', 'c', 'ce</w>', 'ch', 'd', 'd</w>', 'e', 'e</w>', 'ed</w>', 'en', 'er', 'f', 'for', 'g', 'h', 'i', 'in', 'ing</w>', 'k', 'l', 'm', 'model', 'n', 'o', 'o</w>', 'od', 'on', 'on</w>', 'or', 'ou', 'p', 'per', 'r', 're', 'reasoning</w>', 's', 's</w>', 'st', 'su', 't', 't</w>', 'th', 'u', 'v', 'w', 'x', 'y']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:09:26.111854Z",
     "start_time": "2025-08-27T11:09:26.105846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bpe_vocab_sorted = sorted(bpe_vocab, key=len, reverse=True)\n",
    "\n",
    "def bpe_tokenize_fast(_word, _vocab_set, max_len=10):\n",
    "    _tokens = []\n",
    "    _i = 0\n",
    "    while _i < len(_word):\n",
    "        match = None\n",
    "        # 尝试从当前位置匹配最长 token\n",
    "        for l in range(min(max_len, len(_word) - _i), 0, -1):\n",
    "            sub = _word[_i:_i + l]\n",
    "            # 如果在词表里（去掉 </w> 匹配）\n",
    "            if sub in _vocab_set or (sub + '</w>') in _vocab_set:\n",
    "                match = sub\n",
    "                # 如果匹配到完整 token 带 </w>，加上\n",
    "                if (sub + '</w>') in _vocab_set:\n",
    "                    match += '</w>'\n",
    "                break\n",
    "        if match:\n",
    "            _tokens.append(match)\n",
    "            _i += len(match.replace('</w>', ''))\n",
    "        else:\n",
    "            _tokens.append(_word[_i])\n",
    "            _i += 1\n",
    "    return _tokens\n",
    "\n",
    "# 使用\n",
    "vocab_set = set(bpe_vocab)  # 用 set 查找加速\n",
    "text = \"DeepSeek-R1 demonstrates reasoning.\"\n",
    "words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "tokens = []\n",
    "for w in words:\n",
    "    tokens.extend(bpe_tokenize_fast(w, vocab_set))\n",
    "print(tokens)\n"
   ],
   "id": "69424239434a4a65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DeepSeek</w>', '-</w>', 'R1</w>', 'd</w>', 'e</w>', 'm', 'on</w>', 'st', 'r', 'at', 'e</w>', 's</w>', 'reasoning</w>', '.</w>']\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
