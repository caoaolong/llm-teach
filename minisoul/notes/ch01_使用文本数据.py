import marimo

__generated_with = "0.16.2"
app = marimo.App(width="medium", app_title="LLMs")

with app.setup(hide_code=True):
    # Initialization code that runs before all other cells
    import marimo as mo


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    # 1.å¤„ç†é—®æœ¬æ•°æ®

    > æœ¬ç« ä»‹ç»æ•°æ®å‡†å¤‡å’Œé‡‡æ ·ï¼Œä»¥ä¾¿ä¸º LLM å‡†å¤‡è¾“å…¥æ•°æ®

    ---

    ![1-1](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-001.svg)

    ---
    ## 1.1 ç†è§£è¯åµŒå…¥

    > åµŒå…¥æœ‰å¾ˆå¤šç§å½¢å¼ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸»è¦ä»‹ç»æ–‡æœ¬åµŒå…¥

    ![1-2](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-02.svg)

    /// details | :fire: ä»€ä¹ˆæ˜¯*Embedding*
        type: info
    å¤§æ¨¡å‹ä¸­ä½¿ç”¨çš„æ•°æ®åµŒå…¥ç»´åº¦å¾ˆé«˜ï¼Œä»¥è‡³äºæ— æ³•ç”¨ç›´è§‚çš„æ–¹å¼å±•ç°ï¼ˆä¸‰ç»´å·²ç»æ˜¯äººç±»èƒ½å¤Ÿç†è§£çš„æé™ï¼‰ã€‚

    ä½†æ˜¯æˆ‘ä»¬å¯ä»¥ç”¨ä½ç»´åº¦çš„æ•°æ®æ¥å¸®åŠ©æˆ‘ä»¬ç†è§£ä»€ä¹ˆæ˜¯åµŒå…¥ï¼ˆ*Embedding*ï¼‰ã€‚
    ///

    ![1-3](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-03.svg)

    ## 1.2 æ–‡æœ¬æ ‡è®°åŒ–

    > ä¹Ÿå¯ä»¥ç†è§£ä¸ºæ–‡æœ¬åˆ†è¯ï¼Œä¸»è¦å·¥ä½œå°±æ˜¯å°†é•¿æ–‡æœ¬ï¼ˆæ–‡ç« ã€å°è¯´ï¼‰æ‹†åˆ†ä¸ºæ›´å°çš„å•ä½ï¼Œæ¯”å¦‚å•è¯å’Œæ ‡ç‚¹ç¬¦å·ã€‚

    ![1-4](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-04.svg)

    /// details | :hammer: è·å–æ–‡æœ¬æ•°æ®æº
        type: warn
    è¦æƒ³å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œé¦–å…ˆéœ€è¦ä¸€æ®µæ¯”è¾ƒé•¿çš„æ–‡æœ¬ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä¸€ç¯‡è‹±æ–‡å°è¯´ ***[The Verdict](https://en.wikisource.org/wiki/The_Verdict)***ã€‚

    å°†å…¶ä¸‹è½½ä¸‹æ¥ä¿å­˜ä¸ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶`the_verdict.txt`å­˜æ”¾åœ¨ä½ çš„å·¥ç¨‹ç›®å½•ä¸‹ã€‚
    ///
    """
    )
    return


@app.cell
def _():
    with open("./data/the_verdict.txt", "r", encoding="utf-8") as f:
        raw_text = f.read()

    with mo.redirect_stdout():
        print(f"æ€»å­—ç¬¦æ•°: {len(raw_text)}")
        print(f"{raw_text[:50]}...")
    return (raw_text,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    /// details | :hammer: å¦‚ä½•å®ç°*Embedding*

    æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯å°†è¿™æ®µé•¿æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å’ŒåµŒå…¥åŒ–æ“ä½œ

    é¦–å…ˆæ¥çœ‹ä»€ä¹ˆæ˜¯æ ‡è®°åŒ–ï¼Œä»¥ä¸‹æ˜¯ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†ä¸€å¥è¯çš„è¿‡ç¨‹
    ///
    """
    )
    return


@app.cell
def _():
    import re

    re_text = "Hello, world. This, is a test."
    re_result1 = re.split(r"(\s)", re_text)
    re_result2 = re.split(r"([,.]|\s)", re_text)

    mo.md(
        f"""
    :hammer: ç®€å•æ‹†åˆ†: 

    ```python
    {re_result1}
    ```

    /// attention | é—®é¢˜å‡ºç°äº†
    å¯ä»¥çœ‹åˆ°æ•°ç»„ä¸­å‡ºç°äº†å¾ˆå¤šç©ºæ ¼ï¼Œè€Œä¸”ç¬¦å·å’Œå•è¯ç²˜è¿åœ¨ä¸€èµ·äº†ï¼
    ///

    :hammer: å¤æ‚æ‹†åˆ†: 

    ```python
    {re_result2}
    ```
    """
    )
    return re, re_result2


@app.cell
def _(re_result2):
    split_result = [item for item in re_result2 if item.strip()]
    mo.md(
        f"""
    :hammer: å»é™¤ç©ºæ ¼: 
    ```python
    {split_result}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    /// details | :cyclone: å·²ç»å®Œç¾äº†å—ï¼Ÿ

    ç°åœ¨çœ‹èµ·æ¥å·²ç»å¾ˆå®Œç¾äº†ï¼Œä½†å®é™…æˆ‘ä»¬å¾—åˆ°çš„åŸå§‹æ–‡æœ¬å†…å®¹ä¼šæ›´å¤æ‚ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼š

    ```text
    Hello, world. Is this-- a test?
    ```
    å› æ­¤æˆ‘ä»¬éœ€è¦æ›´åŠ å¤æ‚çš„åˆ†å‰²æ–¹å¼ã€‚
    ///
    """
    )
    return


@app.cell
def _(re):
    text = "Hello, world. Is this-- a test?"

    split_result2 = re.split(r'([,.:;?_!"()\']|--|\s)', text)
    list_result = [item.strip() for item in split_result2 if item.strip()]
    mo.md(
        f"""
    :hammer: åˆ†å‰²ç»“æœ: 
    ```python
    {list_result}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    /// admonition | :white_check_mark: æ–‡æœ¬åˆ†å‰²å®Œæˆ

    ç°åœ¨ä½¿ç”¨è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å°±å¯ä»¥å¾ˆå¥½å¾—å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å‰²äº†ã€‚
    ///

    ![1-5](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-05.svg)

    :hammer: æ¥ä¸‹æ¥æŒ‰ç…§è¿™ä¸ªæ€è·¯å¯¹ ***the_verdict.txt*** è¿›è¡Œåˆ†å‰²
    """
    )
    return


@app.cell
def _(raw_text, re):
    preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
    preprocessed = [item.strip() for item in preprocessed if item.strip()]
    mo.md(
        f"""
    åˆ†å‰²åé•¿åº¦ä¸º `{len(preprocessed)}`, Tokenså¦‚ä¸‹ï¼š
    ```python
    {preprocessed[:10]} ...
    ```

    """
    )
    return (preprocessed,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.3 å°† Tokens è½¬æ¢ä¸º Token IDs

    > æˆ‘ä»¬å·²ç»å°†é•¿æ–‡æœ¬æ‹†åˆ†æˆäº† `Token` åºåˆ—ï¼Œä½†æ˜¯è®¡ç®—æœºä»ç„¶æ— æ³•è¯»æ‡‚è¿™äº›å†…å®¹ï¼Œå› æ­¤è¿˜éœ€è¦å°†å®ƒä»¬é‡åŒ–ã€‚

    ![1-6](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-06.svg)

    ä»è¿™äº› `Tokens` ä¸­å¯ä»¥æ•´ç†å‡ºä¸€ä¸ªè¯è¡¨ `vocabulary`ï¼Œæ­¤è¡¨ä¸­ <u>åŒ…å«äº†æ‰€æœ‰ä¸”ä¸é‡å¤</u> çš„ `Token`ã€‚
    """
    )
    return


@app.cell
def _(preprocessed):
    all_words = sorted(set(preprocessed))
    vocab_size = len(all_words)
    vocab = {token: integer for integer, token in enumerate(all_words)}

    mo.ui.table(
        data=[{"token": token, "id": id} for token, id in vocab.items()],
        label=f"Tokens, Length={vocab_size}",
    )
    return (vocab,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ä¸‹å›¾å±•ç¤ºäº†æ–‡æœ¬æ ·æœ¬ç¼–ç ä¸º `Token IDs` çš„è¿‡ç¨‹ï¼š

    ![1-7](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-07.svg)

    /// admonition | :white_check_mark: è¯è¡¨æ„å»ºå®Œæˆ

    è¯è¡¨å·²å°†åˆ›å»ºå®Œæˆäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨è¯è¡¨ä¸­çš„ä¸€å°éƒ¨åˆ†æ ·æœ¬æ¥è¯´æ˜æ–‡æœ¬æ ‡è®°åŒ– `Tokenization`ã€‚

    ///
    """
    )
    return


@app.cell(hide_code=True)
def _(re, vocab):
    class SimpleTokenizerV1:
        def __init__(self, vocab):
            """Token -> TokenIDï¼šå¯¹åº”ç¼–ç è¿‡ç¨‹"""
            self.str_to_int = vocab

            """TokenID -> Tokenï¼šå¯¹åº”è§£ç è¿‡ç¨‹"""
            self.int_to_str = {i: s for s, i in vocab.items()}

        def encode(self, text):
            preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
            preprocessed = [item.strip() for item in preprocessed if item.strip()]
            ids = [self.str_to_int[s] for s in preprocessed]
            return ids

        def decode(self, ids):
            text = " ".join([self.int_to_str[i] for i in ids])
            # Replace spaces before the specified punctuations
            text = re.sub(r'\s+([,.?!"()\'])', r"\1", text)
            return text


    tokenizer_v1 = SimpleTokenizerV1(vocab)
    mo.show_code()
    return (tokenizer_v1,)


@app.cell(disabled=True, hide_code=True)
def _():
    mo.md(
        r"""
    æˆ‘ä»¬ç”¨ä¹‹å‰çš„æ–¹å¼æ„å»ºäº†ä¸€ä¸ªåä¸º `SimpleTokenizerV1` çš„ç®€æ˜“ç‰ˆæ–‡æœ¬æ ‡è®°å™¨ã€‚

    + `str_to_int` å­—æ®µä¿å­˜äº†ä» `Token` åˆ° `TokenID` çš„æ˜ å°„ï¼Œå¯¹åº” <u>ç¼–ç (`encode`)</u> çš„è¿‡ç¨‹
    + `int_to_str` å­—æ®µä¿å­˜äº†ä» `TokenID` åˆ° `Token` çš„æ˜ å°„ï¼Œå¯¹åº” <u>è§£ç (`decode`)</u> çš„è¿‡ç¨‹

    ![1-8](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-08.svg)

    /// details | :hammer: æˆ‘ä»¬å¯ä»¥ç”¨ä¸€æ®µç®€å•çš„æ–‡æœ¬å¯¹è¿™ä¸ªæ–‡æœ¬æ ‡è®°å™¨è¿›è¡Œæµ‹è¯•ã€‚
        type: warn

    **(1/3)** ä½¿ç”¨ä¸€æ®µæ–‡æœ¬ä½œä¸ºè¾“å…¥è¿›è¡Œç¼–ç (`encode`)æ“ä½œ

    **(2/3)** å°†ç¼–ç åçš„æ–‡æœ¬è¿›è¡Œè§£ç (`decode`)æ“ä½œ

    **(3/3)** æ£€æŸ¥è§£ç åçš„æ–‡æœ¬ä¸åŸæ–‡æœ¬æ˜¯å¦ä¸€è‡´
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _(tokenizer_v1):
    simple_text = """
    "It's the last he painted, you know," Mrs. Gisburn said with pardonable pride.
    """
    encoded_value = tokenizer_v1.encode(simple_text)
    decoded_value = tokenizer_v1.decode(encoded_value)

    mo.md(
        f"""
    åŸæ–‡æœ¬ï¼š<br/>
    {simple_text}

    ç¼–ç åçš„å†…å®¹ï¼š
    ```python
    {encoded_value}
    ```

    è§£ç åçš„å†…å®¹ï¼š<br/>
    {decoded_value}

    /// admonition | :white_check_mark: ç¼–-è§£ç å™¨(v1)åˆ›å»ºå®Œæˆ

    ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€ä¸ªç®€æ˜“ç‰ˆ åŸå§‹æ–‡æœ¬å’Œæ ‡è®°æ–‡æœ¬IDè½¬æ¢ çš„å·¥å…·
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    /// attention | é—®é¢˜å‡ºç°äº†
    å¦‚æœè¾“å…¥çš„å­—ç¬¦ä¸­æœ‰äº›å­—ç¬¦å¹¶æ²¡æœ‰åŒ…å«åœ¨è¯è¡¨ä¸­ï¼Œå°±æ— æ³•å°†å…¶æ˜ å°„åˆ° `TokenID`
    ///
    """
    )
    return


@app.cell
def _(tokenizer_v1):
    text_new_token = "Hello, do you like tea. Is this-- a test?"
    tokenizer_v1.encode(text_new_token)
    return (text_new_token,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.4 æ·»åŠ ç‰¹æ®Šçš„ä¸Šä¸‹æ–‡æ ‡è®°

    > åœ¨è¯è¡¨ä¸­å®šä¹‰ä¸€äº›ç‰¹æ®Šçš„æ ‡è®°æ–‡æœ¬æ¥å¼¥è¡¥è¾“å…¥çš„å•è¯æ— æ³•æ˜ å°„åˆ° `TokenID` çš„ç¼ºé™·ã€‚

    ![1-9](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-09.svg)

    äº‹å®ä¸Šï¼Œè¿™äº›ç‰¹æ®Šæ ‡è®°çš„ä½œç”¨ä¸æ­¢å¦‚æ­¤ï¼Œå¾ˆå¤šæ¨¡å‹ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ¥ä¸ºæ¨¡å‹æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚

    /// details | :cyclone: ç‰¹æ®Šæ ‡è®°çš„ä½œç”¨

    :radio_button: `[BOS]` (beginning of sequence) ä½œä¸ºæ–‡æœ¬å¼€å§‹çš„æ ‡è®°<br/>
    :radio_button: `[EOS]` (end of sequence) ä½œä¸ºæ–‡æœ¬ç»“æŸçš„æ ‡è®°<br/>
    :radio_button: `[PAD]` (padding) ç”¨äºåœ¨è®­ç»ƒæ—¶å¯¹æ ·æœ¬è¿›è¡Œé•¿åº¦è¡¥é½<br/>
    :radio_button: `[UNK]` (unknown) ç”¨äºè¡¨ç¤ºè¯è¡¨ä¸­ä¸åŒ…å«çš„å•è¯<br/>
    ///

    ğŸ”¥ è¯·æ³¨æ„ï¼ŒGPT-2æ²¡æœ‰ä½¿ç”¨ä¸Šè¿°ä»»ä½•ä¸€ç§ï¼Œè€Œæ˜¯ä½¿ç”¨`<|endoftext|>`ï¼Œæ—¢ç”¨æ¥ä½œä¸ºæ–‡æœ¬çš„ç»“æŸæ ‡è®°ï¼Œæœ‰ç”¨ä½œè®­ç»ƒæ—¶çš„é•¿åº¦è¡¥é½ã€‚

    æˆ‘ä»¬å°†å‚è€ƒGPT-2ï¼Œä¸¤ä¸ªç‹¬ç«‹çš„æ–‡æœ¬æºä¹‹é—´ä½¿ç”¨ `<|endoftext|>` æ ‡è®°ã€‚

    ![1-10](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-10.svg)
    """
    )
    return


@app.cell
def _(preprocessed):
    all_tokens = sorted(list(set(preprocessed)))
    all_tokens.extend(["<|endoftext|>", "<|unk|>"])
    vocab_v2 = {token: integer for integer, token in enumerate(all_tokens)}

    mo.ui.table(
        data=[{"token": token, "id": id} for token, id in vocab_v2.items()],
        label=f"Tokens, Length={len(vocab_v2)}",
    )
    return (vocab_v2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":hammer: æ¥ä¸‹æ¥è®©æˆ‘ä»¬å®Œå–„ä¸€ä¸‹ `SimpleTokenizerV1`ï¼Œè®©å®ƒå¯ä»¥å¤„ç†è¯è¡¨ä¸­ä¸å­˜åœ¨çš„å•è¯ã€‚"""
    )
    return


@app.cell(hide_code=True)
def _(re, vocab_v2):
    class SimpleTokenizerV2:
        def __init__(self, vocab):
            self.str_to_int = vocab
            self.int_to_str = {i: s for s, i in vocab.items()}

        def encode(self, text):
            preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
            preprocessed = [item.strip() for item in preprocessed if item.strip()]
            preprocessed = [
                item if item in self.str_to_int else "<|unk|>"
                for item in preprocessed
            ]

            ids = [self.str_to_int[s] for s in preprocessed]
            return ids

        def decode(self, ids):
            text = " ".join([self.int_to_str[i] for i in ids])
            # Replace spaces before the specified punctuations
            text = re.sub(r'\s+([,.:;?!"()\'])', r"\1", text)
            return text


    tokenizer_v2 = SimpleTokenizerV2(vocab_v2)

    mo.show_code()
    return (tokenizer_v2,)


@app.cell
def _():
    mo.md(
        r""":hammer: æ¥ä¸‹æ¥è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æ–°çš„ `SimpleTokenizerV2`ï¼Œçœ‹æ˜¯å¦å¯ä»¥å¤„ç†æ­¤è¡¨ä¸­ä¸å­˜åœ¨çš„å•è¯ã€‚"""
    )
    return


@app.cell
def _(text_new_token, tokenizer_v2):
    encoded_values_v2 = tokenizer_v2.encode(text_new_token)
    decoded_values_v2 = tokenizer_v2.decode(encoded_values_v2)

    mo.md(
        f"""
    åŸæ–‡æœ¬ï¼š<br/>
    {text_new_token}

    ç¼–ç åçš„å†…å®¹ï¼š
    ```python
    {encoded_values_v2}
    ```

    è§£ç åçš„å†…å®¹ï¼š<br/>
    {decoded_values_v2}

    /// admonition | :white_check_mark: ç¼–-è§£ç å™¨(v2)åˆ›å»ºå®Œæˆ

    ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€ä¸ªè¾ƒä¸ºå®Œå–„çš„ç¼–è§£ç å™¨
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.5 BytePair encoding

    > `GPT-2`ä½¿ç”¨äº†BytePair encoding(BPE)ç®—æ³•è¿›è¡Œæ–‡æœ¬æ ‡è®°åŒ–å¤„ç†ã€‚

    :rocket: BPEç®—æ³•çš„ä¼˜åŠ¿åœ¨äºå®ƒèƒ½å¤Ÿå°†ä¸å­˜åœ¨äºè¯è¡¨ä¸­çš„å•è¯æ‹†åˆ†ä¸ºæ›´å°çš„å•å…ƒï¼ˆæ¯”å¦‚å•ä¸ªå­—ç¬¦ï¼‰ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è¯è¡¨ä¹‹å¤–çš„å•è¯ã€‚

    + ä¾‹å¦‚ï¼Œå¦‚æœ `GPT-2` çš„è¯æ±‡è¡¨ä¸­æ²¡æœ‰ `"unfamiliarword"` è¿™ä¸ªè¯ï¼Œå®ƒå¯èƒ½ä¼šå°†å…¶æ ‡è®°ä¸º `["unfam"ã€"iliar"ã€"word"]` æˆ–å…¶ä»–å­è¯åˆ†è§£ï¼Œå…·ä½“å–å†³äºå…¶è®­ç»ƒè¿‡çš„ BPE åˆå¹¶è¿‡ç¨‹ã€‚

    + :link: [`GPT-2`çš„ç¼–ç å™¨æºç ](https://github.com/openai/gpt-2/blob/master/src/encoder.py)å¯ä»¥ä»è¿™é‡Œè·å–ï¼

    æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä½¿ç”¨ `OpenAI` çš„å¼€æºåº“ `tiktoken` ä¸­çš„ `BPE` åˆ†è¯å™¨ï¼Œè¯¥åº“ä½¿ç”¨ `Rust` å®ç°ï¼Œæ•ˆç‡éå¸¸é«˜ï¼å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ã€‚

    ```shell
    pip install tiktoken
    ```
    """
    )
    return


@app.cell
def _():
    import importlib
    import tiktoken

    with mo.redirect_stdout():
        print("tiktoken version:", importlib.metadata.version("tiktoken"))
    return (tiktoken,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :hammer: æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹BPEçš„æ‰§è¡Œæ•ˆæœ

    :rocket: BPEç®—æ³•ä¼šå°†æœªçŸ¥çš„å•è¯æ‹†åˆ†ä¸ºæ›´å°çš„å•ä½ï¼ˆè‹¥å¹²ä¸ªå­å•è¯æˆ–è€…å•ä¸ªå­—ç¬¦ï¼‰

    ![1-11](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-11.svg)
    """
    )
    return


@app.cell
def _(tiktoken):
    text_gpt_test = (
        "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
        "of someunknownPlace."
    )
    tokenizer = tiktoken.get_encoding("gpt2")
    # è¿™é‡Œé…ç½®å…è®¸ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°
    gpt_token_ids = tokenizer.encode(
        text_gpt_test, allowed_special={"<|endoftext|>"}
    )
    gpt_decoded_text = tokenizer.decode(gpt_token_ids)

    mo.md(
        f"""
    :tada: `GPT-2` çš„ç¼–ç å™¨

    åŸå§‹æ–‡æœ¬ï¼š
    ```text
    {text_gpt_test}
    ```

    ç¼–ç ç»“æœï¼š
    ```python
    {gpt_token_ids}
    ```

    è§£ç ç»“æœï¼š
    ```text
    {gpt_decoded_text}
    ```

    /// admonition | :white_check_mark: æ¢ç´¢ `GPT-2` çš„ç¼–ç å™¨

    ç°åœ¨æˆ‘ä»¬å·²ç»åˆæ­¥äº†è§£äº† `GPT-2` çš„ç¼–ç å™¨çš„åŸºæœ¬å·¥ä½œåŸç†
    ///
    """
    )
    return (tokenizer,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.6 BPEçš„è®­ç»ƒè¿‡ç¨‹

    > å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encoding, BPEï¼‰æ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬å¤„ç†çš„å‹ç¼©ç®—æ³•ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„è¯æ±‡æ„å»ºã€‚

    ///details | :rocket: åŸºæœ¬è¿‡ç¨‹å¦‚ä¸‹ï¼š
        type: warn
    :radio_button: **(1/4)** æŠŠæ–‡æœ¬åˆ‡åˆ†ä¸ºå•è¯ï¼Œå¹¶åœ¨å•è¯æœ«å°¾åŠ ç‰¹æ®Šæ ‡è®° `</w>`ã€‚<br/>
    :radio_button: **(2/4)** ç»Ÿè®¡æ‰€æœ‰bigramå‡ºç°é¢‘ç‡ã€‚<br/>
    :radio_button: **(3/4)** æ‰§è¡Œåˆå¹¶ã€‚<br/>
    :radio_button: **(4/4)** é‡å¤æ­¥éª¤2ã€3ç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡å¤§å°æˆ–æ— æ³•å†æ‰¾åˆ°é¢‘ç‡é«˜çš„å­—ç¬¦å¯¹ä¸ºæ­¢ã€‚<br/>
    ///

    :page_with_curl: å‡å¦‚ç°æœ‰å¦‚ä¸‹æ–‡æœ¬æ ·æœ¬ï¼š

    ```shell
    # 10ä¸ª
    hug hug hug hug hug hug hug hug hug hug
    # 5ä¸ª
    pug pug pug pug pug
    # 12ä¸ª
    pun pun pun pun pun pun pun pun pun pun pun pun
    # 4ä¸ª
    bun bun bun bun
    # 5ä¸ª
    hugs hugs hugs hugs hugs
    ```

    :hammer: é¦–å…ˆéœ€è¦å°†ä»–ä»¬æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
    """
    )
    return


@app.cell
def _(re):
    from collections import Counter

    bpe_text_sample = """
    hug hug hug hug hug hug hug hug hug hug
    pug pug pug pug pug
    pun pun pun pun pun pun pun pun pun pun pun pun
    bun bun bun bun
    hugs hugs hugs hugs hugs
    """

    bpe_words = re.findall(r"\w+|[^\w\s]", bpe_text_sample)
    bpe_corpus = [" ".join(list(w)) + " </w>" for w in bpe_words]

    bpe_word_freqs = Counter(bpe_corpus)

    mo.md(
        f"""
    :tada: å­—ç¬¦åˆ—è¡¨ï¼š

    + `bpe_word_freqs`
    ```python
    {bpe_word_freqs} ...
    ```
    """
    )
    return Counter, bpe_word_freqs


@app.cell
def _(bpe_word_freqs, re):
    from collections import defaultdict


    def get_stats(word_freqs):
        """ç»Ÿè®¡æ‰€æœ‰bigramå‡ºç°é¢‘ç‡"""
        pairs = defaultdict(int)
        for word, freq in word_freqs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs


    def merge_vocab(pair, v_in):
        """æ‰§è¡Œä¸€æ¬¡åˆå¹¶"""
        v_out = {}
        bigram = re.escape(" ".join(pair))
        p = re.compile(r"(?<!\S)" + bigram + r"(?!\S)")
        for word in v_in:
            w_out = p.sub("".join(pair), word)
            v_out[w_out] = v_in[word]
        return v_out


    bpe_vocab = bpe_word_freqs.copy()

    bpe_first_pairs = get_stats(bpe_vocab)
    bpe_first_best = max(bpe_first_pairs, key=bpe_first_pairs.get)

    mo.md(
        f"""
    :hammer: è®¡ç›¸é‚»ä¸¤ä¸ªå­—ç¬¦å‡ºç°çš„é¢‘ç‡

    ---

    åˆå¹¶å‰ï¼š
    ```python
    {dict(bpe_vocab)}
    ```


    æœ¬æ¬¡åˆå¹¶é¡¹ä¸ºï¼š`{bpe_first_best}`, å‡ºç°é¢‘ç‡ä¸ºï¼š`{bpe_first_pairs[bpe_first_best]}`

    ---

    :tada: å­—ç¬¦é¢‘ç‡ç»Ÿè®¡ç»“æœï¼š
    ```python
    {merge_vocab(bpe_first_best, bpe_vocab)}
    ```
    """
    )
    return get_stats, merge_vocab


@app.cell(hide_code=True)
def _(Counter, bpe_word_freqs, get_stats, merge_vocab):
    get_merge_table, set_merge_table = mo.state([])
    get_merge_entry, set_merge_entry = mo.state("")


    def bpe_merge(count: any, vocab: Counter):
        if type(count) != int:
            return []
        for t in range(count):
            pairs = get_stats(vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get)  # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„bigram
            set_merge_entry(
                f"""æœ¬æ¬¡åˆå¹¶é¡¹ä¸ºï¼š`{best}`, å‡ºç°é¢‘ç‡ä¸ºï¼š`{pairs[best]}`"""
            )
            vocab = merge_vocab(best, vocab)
        return vocab


    bpe_merge_slider = mo.ui.slider(
        start=0,
        stop=10,
        step=1,
        show_value=True,
        label="åˆå¹¶æ¬¡æ•°",
        on_change=lambda value: set_merge_table(
            bpe_merge(value, bpe_word_freqs.copy())
        ),
    )
    return bpe_merge_slider, get_merge_entry, get_merge_table


@app.cell(hide_code=True)
def _(bpe_merge_slider, get_merge_entry, get_merge_table):
    mo.vstack(
        [
            bpe_merge_slider,
            mo.md(get_merge_entry()),
            mo.ui.table(data=get_merge_table()),
            mo.md(
                f"""
    /// admonition | :white_check_mark: BPEç®—æ³•

    ç°åœ¨ï¼Œä½ å·²ç»å®Œå…¨æŒæ¡äº†BPEç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ï¼
            """
            ),
        ]
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.7 æ»‘åŠ¨çª—å£å®ç°æ•°æ®é‡‡æ ·

    > æˆ‘ä»¬è®­ç»ƒ LLM ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå•è¯ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›ç›¸åº”åœ°å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œå°†åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ä½œä¸ºé¢„æµ‹çš„ç›®æ ‡ã€‚

    ![1-12](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-12.svg)
    """
    )
    return


@app.cell
def _(raw_text, tokenizer):
    encoded_text = tokenizer.encode(raw_text)

    mo.md(
        f"""
    ç¼–ç åçš„è¯è¡¨å¤§å°ï¼š`{len(encoded_text)}`

    è¯è¡¨ï¼š
    ```python
    {encoded_text[:10]} ...
    ```
    """
    )
    return (encoded_text,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ///details | :fire: å¦‚ä½•å¤„ç†è¾“å…¥æ–‡æœ¬ï¼Ÿ

    :radio_button: é¦–å…ˆï¼Œæ¯æ¬¡è¾“å…¥ç»™æ¨¡å‹çš„æ–‡æœ¬å—éƒ½éœ€è¦æœ‰ä¸¤éƒ¨åˆ†ï¼ˆå¯ä»¥å‚è€ƒçš„å­—ç¬¦åºåˆ—ï¼Œéœ€è¦é¢„æµ‹çš„ç›®æ ‡å•è¯ï¼‰<br/>
    :radio_button: å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œæ‰€ä»¥ç›®æ ‡æ˜¯å‘å³ç§»åŠ¨ä¸€ä¸ªä½ç½®çš„è¾“å…¥
    ///

    :hammer: æˆ‘ä»¬å‡è®¾çª—å£å¤§å°ä¸º4ï¼Œå·²è¾“å…¥æ–‡æœ¬çš„å‰10ä¸ªå•è¯ä¸¾ä¾‹ï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _(encoded_text):
    # ç”¨äºæ¼”ç¤ºçš„æ–‡æœ¬åºåˆ—
    encoded_sample = encoded_text[:10]
    # ç”¨äºæ¼”ç¤ºçš„æ¨¡å‹çª—å£å¤§å°
    context_size = 4

    mo.show_code()
    return (encoded_sample,)


@app.cell(hide_code=True)
def _(encoded_sample):
    get_refer_sequence, set_refer_sequence = mo.state([])
    get_predict_words, set_predict_words = mo.state([])


    def set_llm_window(value: any):
        if type(value) != int:
            return
        if value > 3:
            set_refer_sequence(encoded_sample[value - 4 : value])
        else:
            set_refer_sequence(encoded_sample[:value])
        set_predict_words(encoded_sample[value : value + 1])


    window_slider = mo.ui.slider(
        start=1,
        stop=9,
        step=1,
        show_value=True,
        label="çª—å£ä½ç½®",
        on_change=set_llm_window,
    )
    return get_predict_words, get_refer_sequence, window_slider


@app.cell(hide_code=True)
def _(encoded_sample, get_predict_words, get_refer_sequence, window_slider):
    mo.vstack(
        [
            window_slider,
            mo.md(
                f"""
    ç”¨äºè®­ç»ƒçš„æ ·æœ¬æ–‡æœ¬å—
    ```python
    {encoded_sample}
    ```
    å‚è€ƒå­—ç¬¦åºåˆ—ï¼š
    ```python
    {get_refer_sequence()}
    ```
    éœ€è¦é¢„æµ‹çš„å­—ç¬¦ï¼š
    ```python
    {get_predict_words()}
    ```

    /// admonition | :white_check_mark: æ¨¡å‹çš„ä¸Šä¸‹æ–‡

    ç°åœ¨ï¼Œä½ å·²ç»å®Œåˆæ­¥äº†è§£äº†å¤§æ¨¡å‹ä¸Šä¸‹æ–‡(`Context`)çš„åŸºæœ¬æ¦‚å¿µ
    """
            ),
        ]
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.8 æ„å»ºè®­ç»ƒæ•°æ®é›†

    > å‚è€ƒæ»‘åŠ¨çª—å£çš„å·¥ä½œåŸç†ï¼Œåˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼Œä»è¾“å…¥æ–‡æœ¬æ•°æ®é›†ä¸­æå–å—

    ![1-13](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-13.svg)

    /// attention | å°æç¤º
    ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬éœ€è¦ç”¨åˆ° `pytorch`ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…

    ```shell
    pip install pytorch
    ```
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _():
    import torch
    from torch.utils.data import Dataset, DataLoader


    class GPTDatasetV1(Dataset):
        """
        txt: æ–‡æœ¬æ•°æ®
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§é•¿åº¦
        stride: å‰è¿›æ­¥å¹…
        """

        def __init__(self, txt, tokenizer, max_length, stride):
            self.input_ids = []
            self.target_ids = []

            # Tokenize the entire text
            token_ids = tokenizer.encode(txt, allowed_special={"<|endoftext|>"})
            assert (
                len(token_ids) > max_length
            ), "æ ‡è®°åŒ–è¾“å…¥çš„æ•°é‡å¿…é¡»è‡³å°‘ç­‰äº max_length+1"

            # Use a sliding window to chunk the book into overlapping sequences of max_length
            for i in range(0, len(token_ids) - max_length, stride):
                input_chunk = token_ids[i : i + max_length]
                target_chunk = token_ids[i + 1 : i + max_length + 1]
                self.input_ids.append(torch.tensor(input_chunk))
                self.target_ids.append(torch.tensor(target_chunk))

        def __len__(self):
            return len(self.input_ids)

        def __getitem__(self, idx):
            return self.input_ids[idx], self.target_ids[idx]


    mo.show_code()
    return DataLoader, GPTDatasetV1, torch


@app.cell(hide_code=True)
def _():
    mo.md(r""":hammer: ç„¶åå†åˆ›å»ºä¸€ä¸ªæ•°æ®é›†åŠ è½½å™¨""")
    return


@app.cell(hide_code=True)
def _(DataLoader, GPTDatasetV1, tiktoken):
    def create_dataloader_v1(
        txt,
        batch_size=4,
        max_length=256,
        stride=128,
        shuffle=True,
        drop_last=True,
        num_workers=0,
    ):

        # Initialize the tokenizer
        tokenizer = tiktoken.get_encoding("gpt2")

        # Create dataset
        dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

        # Create dataloader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            drop_last=drop_last,
            num_workers=num_workers,
        )

        return dataloader


    mo.show_code()
    return (create_dataloader_v1,)


@app.cell(hide_code=True)
def _():
    mo.md(r""":hammer: æ¥ä¸‹æ¥è®©æˆ‘ä»¬æµ‹è¯•ä»¥ä¸‹æ–°å»ºçš„æ•°æ®é›†åŠ è½½å™¨çš„è¿è¡Œæ•ˆæœ""")
    return


@app.cell
def _(create_dataloader_v1, raw_text):
    dataloader_v1 = create_dataloader_v1(
        raw_text, batch_size=1, max_length=4, stride=1, shuffle=False
    )

    dataset_v1_table = []
    for batch in dataloader_v1:
        dataset_v1_table.append(
            {
                "Input TokenIDs": batch[0].numpy(),
                "Input Shape": batch[0].shape,
                "Target TokenIDs": batch[1].numpy(),
                "Target Shape": batch[1].shape,
            }
        )

    mo.ui.table(data=dataset_v1_table)
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ä½¿ç”¨æ­¥å¹…ç­‰äºä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆæ­¤å¤„ä¸º 4ï¼‰çš„ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š

    ![1-14](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-14.svg)

    æˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºæ‰¹é‡è¾“å‡º

    /// attention | å°æç¤º
    è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¢åŠ äº†æ­¥å¹…ï¼Œè¿™æ ·æ‰¹æ¬¡ä¹‹é—´å°±ä¸ä¼šå‡ºç°é‡å ï¼Œå› ä¸ºæ›´å¤šçš„é‡å å¯èƒ½ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆ
    ///

    /// admonition | :white_check_mark: è®­ç»ƒæ•°æ®é›†

    ç°åœ¨ï¼Œä½ å·²ç»æŒæ¡äº†å¦‚ä½•ä½¿ç”¨ `pytorch` æ„å»ºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.9 åˆ›å»ºè¯åµŒå…¥

    > è¯åµŒå…¥å°±æ˜¯ `Token Embeddings`ã€‚æ•°æ®å‡ ä¹å·²ç»ä¸º LLM åšå¥½äº†å‡†å¤‡ï¼Œä½†æœ€åè®©æˆ‘ä»¬ä½¿ç”¨åµŒå…¥å±‚å°†æ ‡è®°åµŒå…¥åˆ°è¿ç»­å‘é‡è¡¨ç¤ºä¸­ï¼Œé€šå¸¸ï¼Œè¿™äº›åµŒå…¥å±‚æ˜¯ LLM æœ¬èº«çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´è¿›è¡Œæ›´æ–°ï¼ˆè®­ç»ƒï¼‰

    ![1-15](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-15.svg)

    :rocket: ä¸ºäº†ç®€å•èµ·è§ï¼Œå‡è®¾æˆ‘ä»¬çš„å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å››ä¸ªè¾“å…¥ç¤ºä¾‹ï¼Œè¾“å…¥ ID åˆ†åˆ«ä¸º 2ã€3ã€5 å’Œ 1ï¼ˆæ ‡è®°åŒ–åï¼‰ï¼Œè¯æ±‡é‡åªæœ‰ `6` ä¸ªå•è¯ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³è¦åˆ›å»ºå¤§å°ä¸º `3` çš„åµŒå…¥ï¼š
    """
    )
    return


@app.cell
def _(torch):
    import torch.nn as nn

    emedding_input_ids = torch.tensor([2, 3, 5, 1])
    example_vocab_size = 6
    output_dim = 3

    torch.manual_seed(123)
    example_embedding_layer = nn.Embedding(example_vocab_size, output_dim)

    mo.md(
        f"""
    Embeddingçš„æƒé‡å¦‚ä¸‹ï¼š
    ```python
    {example_embedding_layer.weight}
    ```
    """
    )
    return emedding_input_ids, example_embedding_layer, nn


@app.cell(hide_code=True)
def _(example_embedding_layer, torch):
    mo.md(
        f"""
    :hammer: è¦å°† ID ä¸º 3 çš„æ ‡è®°è½¬æ¢ä¸ºä¸‰ç»´å‘é‡ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

    ```python
    example_embedding_layer(torch.tensor([3]))
    example_embedding_layer(torch.tensor([5]))
    ```

    è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

    ```python
    3 = {example_embedding_layer(torch.tensor([3]))}
    5 = {example_embedding_layer(torch.tensor([5]))}
    ```
    """
    )
    return


@app.cell
def _(emedding_input_ids, example_embedding_layer):
    mo.md(
        f"""
    :hammer: å‚è€ƒä»¥ä¸Šè¿‡ç¨‹ï¼ŒæŠŠ `emedding_input_ids` å…¨éƒ¨è½¬æ¢ä¸ºå¼ é‡ï¼š

    ```python
    example_embedding_layer(emedding_input_ids))
    ```

    è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

    ```python
    {example_embedding_layer(emedding_input_ids)}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :fire: åµŒå…¥å±‚æœ¬è´¨å…¶å®æ˜¯ä¸€ç§æŸ¥è¯¢æ“ä½œ

    ![1-16](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-16.svg)

    :rocket: åµŒå…¥çš„è¿‡ç¨‹å°±æ˜¯æ ¹æ® `Token ID` ä»æƒé‡çŸ©é˜µæŸ¥æ‰¾å‡ºç›¸åº”ç´¢å¼•å¯¹åº”çš„ `æƒé‡`ã€‚

    /// admonition | :white_check_mark: è¯åµŒå…¥

    ç°åœ¨ï¼Œä½ å·²ç»å®Œå…¨æŒæ¡äº†è¯åµŒå…¥çš„åŸºæœ¬åŸç†ã€‚
    ///
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 1.10 è¯çš„ä½ç½®ç¼–ç 

    > åµŒå…¥å±‚å°† `Token ID` è½¬æ¢ä¸ºç›¸åŒçš„å‘é‡è¡¨ç¤ºï¼Œè¿™ä¸€æ“ä½œå°†ä¼šå¿½è§†æ–‡æœ¬åºåˆ—çš„è¾“å…¥é¡ºåºã€‚

    ![1-17](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-17.svg)

    :rocket: ä½ç½®åµŒå…¥ä¸æ ‡è®°åµŒå…¥å‘é‡ç›¸ç»“åˆï¼Œå½¢æˆå¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å…¥åµŒå…¥ï¼š

    ![1-18](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-18.svg)

    å‡è®¾ `BPE` çš„è¯è¡¨å¤§å°ä¸º `50257`ï¼Œè¾“å…¥çš„å¼ é‡ç»´åº¦ä¸º `256` ç»´ï¼ˆå³ç”¨ `256` ä¸ªç‰¹å¾é‡è¡¨ç¤ºä¸€ä¸ª `Token`ï¼‰
    """
    )
    return


@app.cell
def _(torch):
    token_vocab_size = 50257
    token_output_dim = 256

    token_embedding_layer = torch.nn.Embedding(token_vocab_size, token_output_dim)

    mo.show_code()
    return token_embedding_layer, token_output_dim


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":fire: å¦‚æœæˆ‘ä»¬ä»æ•°æ®åŠ è½½å™¨ä¸­é‡‡æ ·æ•°æ®ï¼Œæˆ‘ä»¬ä¼šå°†æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ ‡è®°åµŒå…¥åˆ°ä¸€ä¸ª 256 ç»´å‘é‡ä¸­ã€‚ å‡è®¾æ‰¹æ¬¡å¤§å°ä¸º 8ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å« 4 ä¸ªæ ‡è®°ï¼Œåˆ™ä¼šäº§ç”Ÿä¸€ä¸ª $8 \times {4} \times {256}$ çš„å¼ é‡ï¼š"""
    )
    return


@app.cell
def _(create_dataloader_v1, raw_text):
    max_length = 4
    dataloader = create_dataloader_v1(
        raw_text,
        batch_size=8,
        max_length=max_length,
        stride=max_length,
        shuffle=False,
    )
    data_iter = iter(dataloader)
    inputs, targets = next(data_iter)

    mo.md(
        f"""
    Token IDs:
    ```python
    {inputs.numpy()}
    ```

    Inputs shape:
    ```python
    {inputs.shape}
    ```
    """
    )
    return inputs, max_length


@app.cell
def _(inputs, token_embedding_layer):
    token_embeddings = token_embedding_layer(inputs)
    with mo.redirect_stdout():
        print(token_embeddings.shape)
    return (token_embeddings,)


@app.cell(hide_code=True)
def _():
    mo.md(r""":rocket: `GPT-2` ä½¿ç”¨ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€åˆ›å»ºå¦ä¸€ä¸ªåµŒå…¥å±‚ï¼š""")
    return


@app.cell
def _(max_length, nn, token_output_dim, torch):
    context_length = max_length
    pos_embedding_layer = nn.Embedding(context_length, token_output_dim)
    pos_embeddings = pos_embedding_layer(torch.arange(max_length))
    with mo.redirect_stdout():
        print(pos_embeddings.shape)
    return (pos_embeddings,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :rocket: è¦åˆ›å»º LLM ä¸­ä½¿ç”¨çš„è¾“å…¥åµŒå…¥ï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ æ ‡è®°å’Œä½ç½®åµŒå…¥ï¼š

    """
    )
    return


@app.cell
def _(pos_embeddings, token_embeddings):
    input_embeddings = token_embeddings + pos_embeddings
    with mo.redirect_stdout():
        print(input_embeddings.shape)
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :rocket: åœ¨è¾“å…¥å¤„ç†å·¥ä½œæµç¨‹çš„åˆå§‹é˜¶æ®µï¼Œè¾“å…¥æ–‡æœ¬ä¼šè¢«åˆ†å‰²æˆå•ç‹¬çš„æ ‡è®°ã€‚ åˆ†å‰²å®Œæˆåï¼Œè¿™äº›æ ‡è®°ä¼šæ ¹æ®é¢„å®šä¹‰çš„è¯æ±‡è¡¨è½¬æ¢ä¸ºæ ‡è®° IDï¼š

    ![1-19](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/1-19.svg)

    /// admonition | :white_check_mark: æ¨¡å‹è¾“å…¥å±‚ï¼ˆ`Token Embedding` å’Œ `Positional Emcoding`ï¼‰

    ç°åœ¨ï¼Œä½ å·²ç»å®Œå…¨æŒæ¡äº† `Transformer` æ¨¡å‹è¾“å…¥å±‚çš„åŸºæœ¬åŸç†
    ///
    """
    )
    return


if __name__ == "__main__":
    app.run()
