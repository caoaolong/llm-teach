{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T06:12:05.374042Z",
     "start_time": "2025-09-09T06:12:05.351511Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "+ 下载数据集\n",
    "\n",
    "```shell\n",
    "modelscope download --dataset caoaolong/zhwiki --local_dir ./minisoul/datasets\n",
    "```"
   ],
   "id": "7c24b50b20786ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:12:07.757257Z",
     "start_time": "2025-09-09T06:12:07.752192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_texts_from_jsonl(file_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    分批读取JSONL文件，避免内存溢出\n",
    "\n",
    "    Args:\n",
    "        file_path: JSONL文件路径\n",
    "        max_samples: 最大样本数，用于测试\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"错误：文件不存在 {file_path}\")\n",
    "        return\n",
    "\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"读取数据\"):\n",
    "            if max_samples and count >= max_samples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                text = f\"{data['title']}\\n{data['content']}\"\n",
    "                yield text\n",
    "                count += 1\n",
    "\n",
    "                # 定期清理内存\n",
    "                if count % 10000 == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON解析错误: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"处理错误: {e}\")\n",
    "                continue"
   ],
   "id": "218785f27e8e5de2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:12:12.282938Z",
     "start_time": "2025-09-09T06:12:12.272940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_tokenizer(data_path='./zhwiki_dataset.jsonl',\n",
    "                   vocab_size=6400,\n",
    "                   max_samples=None,\n",
    "                   output_dir='./model/'):\n",
    "    \"\"\"\n",
    "    训练tokenizer\n",
    "\n",
    "    Args:\n",
    "        data_path: JSONL文件路径\n",
    "        vocab_size: 词汇表大小\n",
    "        max_samples: 最大样本数（用于测试）\n",
    "        output_dir: 输出目录\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"开始训练tokenizer...\")\n",
    "    print(f\"数据文件: {data_path}\")\n",
    "    print(f\"词汇表大小: {vocab_size}\")\n",
    "    print(f\"最大样本数: {max_samples if max_samples else '全部'}\")\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"错误：数据文件不存在 {data_path}\")\n",
    "        print(\"请先运行 create_dataset.py 生成JSONL文件\")\n",
    "        return\n",
    "\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "    # 特殊token\n",
    "    special_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]\n",
    "\n",
    "    # 训练器配置\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "        min_frequency=2  # 最小频率，减少内存使用\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 读取数据\n",
    "        print(\"读取训练数据...\")\n",
    "        texts = read_texts_from_jsonl(data_path, max_samples)\n",
    "\n",
    "        # 训练tokenizer\n",
    "        print(\"开始训练...\")\n",
    "        tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "        # 设置解码器\n",
    "        tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "        # 保存tokenizer\n",
    "        print(\"保存tokenizer...\")\n",
    "        tokenizer.save(os.path.join(output_dir, \"tokenizer.json\"))\n",
    "        tokenizer.model.save(output_dir)\n",
    "\n",
    "        # 创建配置文件\n",
    "        config = {\n",
    "            \"add_bos_token\": False,\n",
    "            \"add_eos_token\": False,\n",
    "            \"add_prefix_space\": False,\n",
    "            \"added_tokens_decoder\": {\n",
    "                \"0\": {\n",
    "                    \"content\": \"<|endoftext|>\",\n",
    "                    \"lstrip\": False,\n",
    "                    \"normalized\": False,\n",
    "                    \"rstrip\": False,\n",
    "                    \"single_word\": False,\n",
    "                    \"special\": True\n",
    "                },\n",
    "                \"1\": {\n",
    "                    \"content\": \"<|im_start|>\",\n",
    "                    \"lstrip\": False,\n",
    "                    \"normalized\": False,\n",
    "                    \"rstrip\": False,\n",
    "                    \"single_word\": False,\n",
    "                    \"special\": True\n",
    "                },\n",
    "                \"2\": {\n",
    "                    \"content\": \"<|im_end|>\",\n",
    "                    \"lstrip\": False,\n",
    "                    \"normalized\": False,\n",
    "                    \"rstrip\": False,\n",
    "                    \"single_word\": False,\n",
    "                    \"special\": True\n",
    "                }\n",
    "            },\n",
    "            \"additional_special_tokens\": [],\n",
    "            \"bos_token\": \"<|im_start|>\",\n",
    "            \"clean_up_tokenization_spaces\": False,\n",
    "            \"eos_token\": \"<|im_end|>\",\n",
    "            \"legacy\": True,\n",
    "            \"model_max_length\": 8192,\n",
    "            \"pad_token\": \"<|endoftext|>\",\n",
    "            \"sp_model_kwargs\": {},\n",
    "            \"spaces_between_special_tokens\": False,\n",
    "            \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "            \"unk_token\": \"<|endoftext|>\",\n",
    "            \"chat_template\": \"\"\"\n",
    "{% if messages[0]['role'] == 'system' %}\n",
    "{% set system_message = messages[0]['content'] %}\n",
    "{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}\n",
    "{% else %}\n",
    "{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}\n",
    "{% endif %}\n",
    "{% for message in messages %}\n",
    "{% set content = message['content'] %}\n",
    "{% if message['role'] == 'user' %}\n",
    "{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "{{ content + '<|im_end|>' + '\\\\n' }}\n",
    "{% endif %}{% endfor %}\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "        # 保存配置文件\n",
    "        config_path = os.path.join(output_dir, \"tokenizer_config.json\")\n",
    "        with open(config_path, \"w\", encoding=\"utf-8\") as config_file:\n",
    "            json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"✅ Tokenizer训练完成！\")\n",
    "        print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "        # 显示统计信息\n",
    "        vocab_size_actual = len(tokenizer.get_vocab())\n",
    "        print(f\"实际词汇表大小: {vocab_size_actual}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 训练失败: {e}\")\n",
    "        raise"
   ],
   "id": "e93ad70e7307a7a3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:29:15.106883Z",
     "start_time": "2025-09-09T06:29:15.100657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_tokenizer(model_dir='../model/'):\n",
    "    \"\"\"\n",
    "    测试训练好的tokenizer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        print(\"测试tokenizer...\")\n",
    "\n",
    "        # 加载tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "        # 测试聊天模板\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "            {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "            {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "        ]\n",
    "\n",
    "        new_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False\n",
    "        )\n",
    "        print(\"聊天模板测试:\")\n",
    "        print(new_prompt)\n",
    "\n",
    "        # 统计信息\n",
    "        actual_vocab_size = len(tokenizer)\n",
    "        print(f'Tokenizer实际词表长度: {actual_vocab_size}')\n",
    "\n",
    "        # 编码测试\n",
    "        model_inputs = tokenizer(new_prompt)\n",
    "        print(f'编码长度: {len(model_inputs[\"input_ids\"])}')\n",
    "\n",
    "        # 解码测试\n",
    "        input_ids = model_inputs['input_ids']\n",
    "        response = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "        print(f'解码和原始文本是否一致: {response == new_prompt}')\n",
    "\n",
    "        print(\"✅ Tokenizer测试通过！\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tokenizer测试失败: {e}\")"
   ],
   "id": "f5a5fe4083d8c6af",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T06:29:45.515035Z",
     "start_time": "2025-09-09T06:29:40.020686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_tokenizer(\n",
    "        data_path='../datasets/zhwiki_dataset.jsonl',\n",
    "        vocab_size=6400,\n",
    "        max_samples=10000,  # 只使用1万个样本进行测试\n",
    "        output_dir='../model/'\n",
    "    )\n",
    "\n",
    "    # 测试tokenizer\n",
    "    eval_tokenizer()"
   ],
   "id": "d598022156f5f97f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练tokenizer...\n",
      "数据文件: ../datasets/zhwiki_dataset.jsonl\n",
      "词汇表大小: 6400\n",
      "最大样本数: 10000\n",
      "读取训练数据...\n",
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取数据: 10000it [00:00, 18294.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存tokenizer...\n",
      "✅ Tokenizer训练完成！\n",
      "输出目录: ../model/\n",
      "实际词汇表大小: 6400\n",
      "测试tokenizer...\n",
      "聊天模板测试:\n",
      "\n",
      "<|im_start|>system\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "你来自哪里？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "我来自地球<|im_end|>\n",
      "\n",
      "            \n",
      "Tokenizer实际词表长度: 6400\n",
      "编码长度: 68\n",
      "解码和原始文本是否一致: True\n",
      "✅ Tokenizer测试通过！\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
