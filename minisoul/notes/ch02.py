import marimo

__generated_with = "0.17.0"
app = marimo.App(width="medium", app_title="LLMs-ç¼–å†™æ³¨æ„åŠ›æœºåˆ¶")

with app.setup(hide_code=True):
    # Initialization code that runs before all other cells
    import marimo as mo
    import torch
    import torch.nn as nn


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    # 2. ç¼–å†™æ³¨æ„åŠ›æœºåˆ¶
    > æœ¬ç« ä»‹ç»æ•° `Transformer` æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ`Attention Mechanisms`ï¼‰ã€‚

    ---

    ![2-1](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-1.svg)


    å®ç°çš„å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š

    ![2-2](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-2.svg)

    ---

    ## 2.1 é•¿åºåˆ—æ¨¡å‹çš„é—®é¢˜

    > ç”±äºæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€çš„è¯­æ³•ç»“æ„å­˜åœ¨å·®å¼‚ï¼Œé€å­—ç¿»è¯‘æ–‡æœ¬æ˜¯ä¸å¯è¡Œçš„ã€‚

    ![2-3](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-3.svg)

    åœ¨ `Transformer` æ¨¡å‹å‡ºç°ä¹‹å‰ï¼Œç¼–ç å™¨-è§£ç å™¨ RNN é€šå¸¸ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚

    åœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œç¼–ç å™¨å¤„ç†æºè¯­è¨€çš„ä¸€ç³»åˆ—æ ‡è®°ï¼Œå¹¶ä½¿ç”¨éšè—çŠ¶æ€ï¼ˆç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§ä¸­é—´å±‚ï¼‰ç”Ÿæˆæ•´ä¸ªè¾“å…¥åºåˆ—çš„ç²¾ç®€è¡¨ç¤ºï¼š

    ![2-4](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-4.svg)

    ## 2.2 æ³¨æ„åŠ›æœºåˆ¶ä»‹ç»

    > é€šè¿‡ä»¥ä¸Šç¿»è¯‘è¿‡ç¨‹å¯ä»¥çœ‹å‡ºï¼Œå†ä¸€æ¬¡ç¿»è¯‘ä¸­ï¼Œå¥å­ä¸­çš„æ¯ä¸ªå•è¯é‡è¦ç¨‹åº¦æ˜¯ä¸åŒï¼Œå¯¹äºæŸäº›å•è¯æˆ‘ä»¬éœ€è¦æ ¼å¤–å…³æ³¨ã€‚è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶æ‰€å®Œæˆçš„ä»»åŠ¡ã€‚

    ![2-5](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-5.svg)

    `Transformer` ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºè¾“å…¥è¡¨ç¤ºçš„æŠ€æœ¯ï¼Œå®ƒä½¿åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®èƒ½å¤Ÿä¸åŒä¸€åºåˆ—ä¸­å…¶ä»–æ¯ä¸ªä½ç½®äº’åŠ¨å¹¶ç¡®å®šå…¶ç›¸å…³æ€§

    ![2-6](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-6.svg)

    ## 2.3 ç®€å•è‡ªæ³¨æ„åŠ›æœºåˆ¶

    ### 2.3.1 è‡ªæ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹

    > æ¢ç´¢ä¸€ä¸ªä¸å¯è®­ç»ƒçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æƒé‡è®¡ç®—è¿‡ç¨‹ã€‚

    :cloud: å‡å¦‚ç°åœ¨è¦è®¡ç®— `journey` å•è¯è¿™è¿™å¥è¯ä¸­çš„é‡è¦ç¨‹åº¦ï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.image(
        src="https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-07.svg",
        width=1440,
    )
    return


@app.function(hide_code=True)
def inputs_table_data(inputs_words, inputs):
    data = []
    words = inputs_words.split(" ")
    for i, item in enumerate(inputs):
        npv = inputs[i].numpy()
        vector = f"[{str(npv[0])}, {str(npv[1])}, {str(npv[2])}]"
        data.append({"ID": i, "Word": words[i], "Vector": vector})
    return data


@app.cell(hide_code=True)
def _():
    inputs_words = "Your journey starts with one step"

    inputs = torch.tensor(
        [
            [0.43, 0.15, 0.89],  # Your     (x^1)
            [0.55, 0.87, 0.66],  # journey  (x^2)
            [0.57, 0.85, 0.64],  # starts   (x^3)
            [0.22, 0.58, 0.33],  # with     (x^4)
            [0.77, 0.25, 0.10],  # one      (x^5)
            [0.05, 0.80, 0.55],  # step     (x^6)
        ]
    )

    mo.ui.table(data=inputs_table_data(inputs_words, inputs))
    return (inputs,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    å›¾ä¸­å±•ç¤ºäº†è¯¥è¿‡ç¨‹çš„åˆå§‹æ­¥éª¤ï¼Œå³é€šè¿‡ç‚¹ç§¯è¿ç®—è®¡ç®— $x^{(2)}$ ä¸æ‰€æœ‰å…¶ä»–è¾“å…¥å…ƒç´ ä¹‹é—´çš„æ³¨æ„åŠ›å¾—åˆ† $\omega$

    ![2-8](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-8.svg)
    """
    )
    return


@app.cell
def _():
    0.55 * 0.43 + 0.87 * 0.15 + 0.66 * 0.89
    return


@app.cell
def _():
    def attn_scores_2v(inputs, query):
        scores = torch.empty(inputs.shape[0])
        for i, x_i in enumerate(inputs):
            scores[i] = torch.dot(
                x_i, query
            )  # dot product (transpose not necessary here since they are 1-dim vectors)
        return scores


    def attn_scores_2(inputs, query):
        result = []
        scores = torch.empty(inputs.shape[0])
        for i, x_i in enumerate(inputs):
            scores[i] = torch.dot(
                x_i, query
            )  # dot product (transpose not necessary here since they are 1-dim vectors)
            value = scores[i].numpy()
            result.append(
                f":radio_button: $Score_{{1{i}}} = Q_1 \cdot X_{i}$ = {value:.4}"
            )
        return "<br/>".join(result)
    return attn_scores_2, attn_scores_2v


@app.cell(hide_code=True)
def _(attn_scores_2, inputs):
    query = inputs[1]  # 2nd input token is the query "journey"

    mo.md(
        f"""
    {attn_scores_2(inputs, query)}

    :fire: ç‚¹ç§¯æœ¬è´¨ä¸Šæ˜¯å°†ä¸¤ä¸ªå‘é‡å…ƒç´ ç›¸ä¹˜ï¼Œç„¶åå¯¹æ‰€å¾—ä¹˜ç§¯æ±‚å’Œçš„ç®€å†™
    """
    )
    return (query,)


@app.cell
def _(inputs, query):
    def test_dot(qidx):
        res = 0.0
        for idx, element in enumerate(inputs[qidx]):
            res += inputs[qidx][idx] * query[idx]
        return res


    _tmp_r = []
    for qidx in range(len(inputs)):
        _tmp_r.append(test_dot(qidx))

    mo.md(
        f"""
    "journey"åœ¨æ¯ä¸ªå•è¯ä¸Šçš„æ³¨æ„åŠ›åˆ†æ•°ï¼š
    ```python
    {torch.tensor(_tmp_r)}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    å¯ä»¥çœ‹åˆ°æ¯ä¸ªè¯æ±‡ç›¸å¯¹äºè¾“å…¥çš„æŸ¥è¯¢ `"journey"` çš„æ³¨æ„åŠ›åˆ†æ•°è¢«è®¡ç®—å‡ºæ¥ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªé—®é¢˜ï¼š

    /// attention | é—®é¢˜å‡ºç°äº†
    è®¡ç®—å‡ºæ¥çš„å€¼æ²¡æœ‰ä¸€ä¸ªå›ºå®šèŒƒå›´ï¼Œ**å’Œ**é¡µæ˜¯ä¸å›ºå®šçš„ã€‚
    ///

    :hammer: ä¸ºäº†ä¾¿äºè®¡ç®—æ¯ä¸ªè¯æ±‡çš„æ³¨æ„åŠ›å æ¯”ï¼Œéœ€è¦å°†å…¶å½’ä¸€åŒ–

    ![2-9](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-9.svg)
    """
    )
    return


@app.cell
def _(attn_scores_2v, inputs, query):
    attn_scores_2_value = attn_scores_2v(inputs, query)
    attn_weights_2_tmp = attn_scores_2_value / attn_scores_2_value.sum()

    mo.md(
        f"""
    å½’ä¸€åŒ–ç»“æœä¸ºï¼š
    ```python
    {attn_weights_2_tmp}
    ```

    å¼ é‡æ€»å’Œä¸º `{attn_weights_2_tmp.sum():.4}`
    """
    )
    attn_scores_2_value = attn_scores_2v(inputs, query)
    attn_weights_2_tmp = attn_scores_2_value / attn_scores_2_value.sum()

    mo.md(
        f"""
    å½’ä¸€åŒ–ç»“æœä¸ºï¼š
    ```python
    {attn_weights_2_tmp}
    ```

    å¼ é‡æ€»å’Œä¸º `{attn_weights_2_tmp.sum():.4}`
    """
    )
    return (attn_scores_2_value,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":rocket: ä»¥ä¸Šä»£ç æ˜¯ä¸€ç§æœ€ç®€å•çš„å½’ä¸€åŒ–æ–¹å¼ï¼Œä½†æ˜¯åœ¨å®è·µä¸­ï¼Œä½¿ç”¨ `softmax` å‡½æ•°è¿›è¡Œå½’ä¸€åŒ–æ˜¯å¸¸è§çš„ï¼Œä¹Ÿæ˜¯æ¨èçš„åšæ³•ï¼Œå› ä¸ºå®ƒæ›´æ“…é•¿å¤„ç†æå€¼ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…·æœ‰æ›´ç†æƒ³çš„æ¢¯åº¦ç‰¹æ€§ã€‚"""
    )
    return


@app.cell
def _(attn_scores_2_value):
    def softmax_naive(x):
        return torch.exp(x) / torch.exp(x).sum(dim=0)


    attn_weights_2_naive = softmax_naive(attn_scores_2_value)

    mo.md(
        f"""
    `Softmax` å½’ä¸€åŒ–ç»“æœä¸ºï¼š
    ```python
    {attn_weights_2_naive}
    ```

    å¼ é‡æ€»å’Œä¸º `{attn_weights_2_naive.sum():.4}`
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :cloud: ä½†æ˜¯ä»¥ä¸Šæ–¹å¼ä»ç„¶ä¼šæœ‰é—®é¢˜ã€‚

    /// attention | é—®é¢˜å‡ºç°äº†
    ç”±äºæº¢å‡ºå’Œä¸‹æº¢é—®é¢˜ï¼Œä¸Šè¿°ç®€å•çš„å®ç°å¯èƒ½ä¼šåœ¨è¾“å…¥å€¼è¾ƒå¤§æˆ–è¾ƒå°æ—¶å‡ºç°æ•°å€¼ä¸ç¨³å®šçš„é—®é¢˜ã€‚
    ///

    :rocket: å› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œå»ºè®®ä½¿ç”¨ PyTorch çš„ softmax å®ç°ï¼Œè¯¥å®ç°å·²é’ˆå¯¹æ€§èƒ½è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(attn_scores_2_value):
    attn_weights_2 = torch.softmax(attn_scores_2_value, dim=0)

    mo.md(
        f"""
    `torch.softmax` å½’ä¸€åŒ–ç»“æœä¸ºï¼š
    ```python
    {attn_weights_2}
    ```

    å¼ é‡æ€»å’Œä¸º `{attn_weights_2.sum():.4}`
    """
    )
    return (attn_weights_2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ $ğ‘§^{(2)}$ æ˜¯é€šè¿‡å°†åµŒå…¥çš„è¾“å…¥æ ‡è®° $ğ‘¥^{(i)}$ ä¸æ³¨æ„åŠ›æƒé‡ç›¸ä¹˜ï¼Œå¹¶å°†å¾—åˆ°çš„å‘é‡ç›¸åŠ ï¼š

    ![2-10](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-10.svg)
    """
    )
    return


@app.cell
def _(attn_weights_2, inputs):
    _tmp_query = inputs[1]  # 2nd input token is the query

    _tmp_context_vec_2 = torch.zeros(_tmp_query.shape)
    for _tmp_i, _tmp_x_i in enumerate(inputs):
        print(f"{attn_weights_2[_tmp_i].numpy():.4} * {_tmp_x_i.numpy()}")
        _tmp_context_vec_2 += attn_weights_2[_tmp_i] * _tmp_x_i

    print(_tmp_context_vec_2)
    return


@app.cell
def _():
    (
        0.1385 * 0.43
        + 0.2379 * 0.55
        + 0.2333 * 0.57
        + 0.124 * 0.22
        + 0.1082 * 0.77
        + 0.1581 * 0.05
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ### 2.3.2 è®¡ç®—æ³¨æ„åŠ›æƒé‡

    > æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ­¤è®¡ç®—æ¨å¹¿åˆ°è®¡ç®—æ‰€æœ‰æ³¨æ„åŠ›æƒé‡å’Œä¸Šä¸‹æ–‡å‘é‡ã€‚

    ![2-11](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-11.svg)

    /// details | :fire: è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡çš„è¿‡ç¨‹

    :radio_button: **(1/3)** åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé¦–å…ˆè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚<br/>
    :radio_button: **(2/3)** ç„¶åå¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—å‡ºæ€»æ³¨æ„åŠ›æƒé‡ä¸º 1 çš„æ³¨æ„åŠ›æƒé‡ã€‚<br/>
    :radio_button: **(3/3)** æœ€åé€šè¿‡å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ã€‚
    ///
    """
    )
    return


@app.cell
def _(inputs):
    attn_scores_native = torch.empty(6, 6)

    for i, x_i in enumerate(inputs):
        for j, x_j in enumerate(inputs):
            attn_scores_native[i, j] = torch.dot(x_i, x_j)

    mo.md(
        f"""
    **(1/3)** æ‰€æœ‰å•è¯çš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ç»“æœï¼š
    ```python
    {attn_scores_native}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(r""":hammer: æˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µä¹˜æ³•æ›´æœ‰æ•ˆåœ°å®ç°ä¸Šè¿°ç›®æ ‡ï¼š""")
    return


@app.cell(hide_code=True)
def _(inputs):
    attn_scores = inputs @ inputs.T

    mo.md(
        f"""
    **(1/3)** æ‰€æœ‰å•è¯çš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ç»“æœï¼š
    ```python
    {attn_scores}
    ```
    """
    )
    return (attn_scores,)


@app.cell
def _(attn_scores):
    attn_weights = torch.softmax(attn_scores, dim=-1)

    mo.md(
        f"""
    **(2/3)** å°†è®¡ç®—ç»“æœå½’ä¸€åŒ–çš„ç»“æœï¼š
    ```python
    {attn_weights}
    ```

    å¯ä»¥è®¡ç®—å¾—å‡ºæ¯ä¸ªå•è¯çš„æ³¨æ„åŠ›æƒé‡æ€»å’Œä¸º `1`ï¼š
    ```python
    {attn_weights.sum(dim=-1)}
    ```
    """
    )
    return (attn_weights,)


@app.cell(hide_code=True)
def _(attn_weights, inputs):
    all_context_vecs = attn_weights @ inputs

    mo.md(
        f"""
    **(3/3)** åŠ æƒæ±‚å’Œï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ï¼š
    ```python
    {all_context_vecs}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.hstack(
        [
            mo.mermaid(
                """
    graph TB;
    S1("ç¡®å®š$$Query(Q)$$") --> S2("è®¡ç®—ç‚¹ç§¯$$(\\omega = Q \\cdot X)$$") --> S3("å½’ä¸€åŒ–$$(\\alpha = Softmax(\\omega))$$") --> S4("è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡$$(Z = \\sum_{i=0}^{T} \\alpha_i X^{i})$$")
    """
            ),
            mo.mermaid(
                """
    graph TB;
    S1("$$Q\\in\\mathbb{R}^{1 \\times 3}$$") --> S2("$$\\omega = Q \\cdot X, X\\in\\mathbb{R}^{1 \\times 3}, \\omega\\in\\mathbb{R}$$") --> S3("$$\\omega_{0-T}=\\{\\omega_0, \\omega_1, ..., \\omega_T\\},\\omega_{0-T}\\in\\mathbb{R}^{1 \\times 6}$$") --> S4("$$\\alpha_{0-T} = Softmax(\\omega_{0-T}), \\alpha_{0-T}\\in\\mathbb{R}^{1 \\times 6}$$") --> S5("$$Z_j = \\sum_{i=0}^{T}\\alpha_i X^{(i)},X\\in\\mathbb{R}^{1 \\times 3},Z\\in\\mathbb{R}^{1 \\times 3}$$") --> S6("$$Z_{0-T}=\\{Z_0,Z_1,...,Z_T\\},Z\\in\\mathbb{R}^{6 \\times 3}$$")
    """
            ),
        ],
        justify="center",
        gap=5,
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 2.4 å®ç°å¯è®­ç»ƒçš„æ³¨æ„åŠ›æƒé‡

    > æœ¬èŠ‚å†…å®¹ä»‹ç»å¦‚ä½•å®ç°ä¸€ä¸ªå¯ä»¥è®­ç»ƒçš„æ³¨æ„åŠ›æœºåˆ¶

    ![2-12](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-12.svg)

    ### 2.4.1 é€æ­¥è®¡ç®—æ³¨æ„åŠ›æƒé‡

    > åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°åŸå§‹ Transformer æ¶æ„ã€GPT æ¨¡å‹å’Œå¤§å¤šæ•°å…¶ä»–æµè¡Œ LLM ä¸­ä½¿ç”¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹Ÿè¢«ç§°ä¸º "ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›" ï¼ˆ`scaled dot-product attention`ï¼‰

    + :cloud: æ€»ä½“æ€è·¯ä¸ä¹‹å‰ç±»ä¼¼ï¼šæˆ‘ä»¬å¸Œæœ›å°†ä¸Šä¸‹æ–‡å‘é‡è®¡ç®—ä¸ºç‰¹å®šè¾“å…¥å…ƒç´ çš„è¾“å…¥å‘é‡çš„åŠ æƒå’Œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„åŠ›æƒé‡ã€‚
    + :cloud: ä¸ä¹‹å‰ä»‹ç»çš„åŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼Œåªæœ‰ç»†å¾®çš„å·®åˆ«ï¼šæœ€æ˜¾ç€çš„åŒºåˆ«æ˜¯å¼•å…¥äº†åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æ›´æ–°çš„æƒé‡çŸ©é˜µã€‚
    + :cloud: è¿™äº›å¯è®­ç»ƒçš„æƒé‡çŸ©é˜µè‡³å…³é‡è¦ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ ç”Ÿæˆâ€œ**è‰¯å¥½**â€çš„ä¸Šä¸‹æ–‡å‘é‡ã€‚

    ![2-13](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-13.svg)

    :rocket: è¦é€æ­¥å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ¥ä»‹ç»ä¸€ä¸‹ä¸‰ä¸ªå¯è®­ç»ƒçš„æƒé‡çŸ©é˜µ$W_q, W_k, W_v$ã€‚è¿™ä¸‰ä¸ªçŸ©é˜µç”¨äºå°†åµŒå…¥çš„è¾“å…¥æ ‡è®° $x^{(i)}$é€šè¿‡çŸ©é˜µä¹˜æ³•æŠ•å½±åˆ°æŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡ä¸­ï¼š

    /// admonition | $QKVçŸ©é˜µ$

    åœ¨æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒQKV çŸ©é˜µé€šå¸¸ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ç›¸å…³è”ã€‚ä»¥ä¸‹æ˜¯ Qã€K å’Œ V çŸ©é˜µå„è‡ªçš„å«ä¹‰åŠå…¶ä½œç”¨ï¼š

    #### 1. Q çŸ©é˜µï¼ˆæŸ¥è¯¢çŸ©é˜µï¼‰

    - **å«ä¹‰**ï¼šQ ä»£è¡¨æŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œå®ƒæ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºï¼Œé€šå¸¸é€šè¿‡å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°ã€‚
    - **ä½œç”¨**ï¼šQ çŸ©é˜µç”¨äºè®¡ç®—è¾“å…¥åºåˆ—ä¸­å„ä¸ªä½ç½®ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡ä¸ K çŸ©é˜µè¿›è¡Œç‚¹ç§¯æ¥ç”Ÿæˆæ³¨æ„åŠ›æƒé‡ï¼Œå†³å®šäº†æ¯ä¸ªä½ç½®å¯¹å…¶ä»–ä½ç½®çš„å…³æ³¨ç¨‹åº¦ã€‚

    #### 2. K çŸ©é˜µï¼ˆé”®çŸ©é˜µï¼‰

    - **å«ä¹‰**ï¼šK ä»£è¡¨é”®ï¼ˆKeyï¼‰ï¼Œå®ƒåŒæ ·æ˜¯é€šè¿‡å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚K çŸ©é˜µæä¾›äº†æ¯ä¸ªä½ç½®çš„â€œæ ‡è¯†â€æˆ–â€œæ ‡ç­¾â€ã€‚
    - **ä½œç”¨**ï¼šK çŸ©é˜µç”¨äºå’Œ Q çŸ©é˜µè¿›è¡ŒåŒ¹é…ï¼Œä»¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ã€‚é€šè¿‡è®¡ç®— Q å’Œ K çš„ç‚¹ç§¯ï¼Œæ¨¡å‹å¯ä»¥è¯„ä¼°æŸ¥è¯¢ä¸å„ä¸ªé”®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œç¡®å®šæ³¨æ„åŠ›çš„åˆ†é…ã€‚

    #### 3. V çŸ©é˜µï¼ˆå€¼çŸ©é˜µï¼‰

    - **å«ä¹‰**ï¼šV ä»£è¡¨å€¼ï¼ˆValueï¼‰ï¼Œå®ƒæ˜¯è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„å®é™…ä¿¡æ¯è¡¨ç¤ºï¼Œä¹Ÿæ˜¯é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚
    - **ä½œç”¨**ï¼šV çŸ©é˜µåŒ…å«äº†å®é™…éœ€è¦ä¼ é€’çš„ä¿¡æ¯ã€‚åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼ŒQ å’Œ K çš„ç‚¹ç§¯ç»“æœï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰ä¼šç”¨äºåŠ æƒ V çŸ©é˜µä¸­çš„å€¼ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºè¡¨ç¤ºã€‚æ¢å¥è¯è¯´ï¼ŒV çŸ©é˜µæä¾›äº†æ ¹æ®æ³¨æ„åŠ›æƒé‡åŠ æƒåçš„ä¿¡æ¯ã€‚

    #### æ€»ç»“

    - **Q çŸ©é˜µ**ï¼šç”¨äºæŸ¥è¯¢ï¼Œè®¡ç®—ä¸ K çš„ç›¸å…³æ€§ã€‚
    - **K çŸ©é˜µ**ï¼šç”¨äºåŒ¹é…æŸ¥è¯¢ï¼Œæä¾›æ¯ä¸ªä½ç½®çš„æ ‡è¯†ã€‚
    - **V çŸ©é˜µ**ï¼šåŒ…å«å®é™…çš„ä¿¡æ¯ï¼Œæ ¹æ®æ³¨æ„åŠ›æƒé‡åŠ æƒåç”Ÿæˆè¾“å‡ºã€‚

    #### è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æµç¨‹

    1. **è¾“å…¥**ï¼šè¾“å…¥åºåˆ—ç»è¿‡çº¿æ€§å˜æ¢å¾—åˆ° Qã€Kã€V çŸ©é˜µã€‚
    2. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼šé€šè¿‡ç‚¹ç§¯è®¡ç®— Q å’Œ K çš„ç›¸ä¼¼åº¦ï¼Œå¹¶é€šè¿‡ Softmax å‡½æ•°å¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚
    3. **åŠ æƒæ±‚å’Œ**ï¼šå°†æ³¨æ„åŠ›æƒé‡åº”ç”¨äº V çŸ©é˜µï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

    è¿™ç§æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†è¾“å…¥åºåˆ—æ—¶ï¼Œè‡ªé€‚åº”åœ°å…³æ³¨ä¸åŒä½ç½®çš„ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚

    + $Query^{(i)} = x^{(i)} \times W_{q}^{(i)}$
    + $Key^{(i)} = x^{(i)} \times W_{k}^{(i)}$
    + $Value^{(i)} = x^{(i)} \times W_{v}^{(i)}$

    + ç®—æ³•è§’åº¦

    ![2-26](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/Example-01.png)

    + åº”ç”¨è§’åº¦

    ![2-26](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-27.webp)

    ///

    :hammer: è¾“å…¥å’ŒæŸ¥è¯¢å‘é‡çš„åµŒå…¥ç»´åº¦å¯ä»¥ç›¸åŒæˆ–ä¸åŒï¼Œè¿™å–å†³äºæ¨¡å‹çš„è®¾è®¡å’Œå…·ä½“å®ç°ã€‚åœ¨ GPT æ¨¡å‹ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºç»´åº¦é€šå¸¸ç›¸åŒï¼Œä½†ä¸ºäº†ä¾¿äºè¯´æ˜ï¼Œä¸ºäº†æ›´å¥½åœ°è·Ÿè¸ªè®¡ç®—ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé€‰æ‹©ä¸åŒçš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _(inputs):
    x_2 = inputs[1]  # second input element
    d_in = inputs.shape[1]  # the input embedding size, d=3
    d_out = 2  # the output embedding size, d=2

    print(x_2)

    mo.show_code()
    return d_in, d_out, x_2


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    $$
    A(A\in\mathbb{R}^{M \times N}) \times B(B\in\mathbb{R}^{N \times P}) = C(C\in\mathbb{R}^{M \times P})
    $$

    è€ƒè™‘ä¸¤ä¸ªçŸ©é˜µ \( A \) å’Œ \( B \)ï¼š

    \[
    A = \begin{pmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6
    \end{pmatrix}, \quad
    B = \begin{pmatrix}
    7 & 8 \\
    9 & 10 \\
    11 & 12
    \end{pmatrix}
    \]

    è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

    \[
    C = A \times B = \begin{pmatrix}
    (1 \cdot 7) + (2 \cdot 9) + (3 \cdot 11) = 7 + 18 + 33 = 58 & (1 \cdot 8) + (2 \cdot 10) + (3 \cdot 12) = 8 + 20 + 36 = 64 \\
    (4 \cdot 7) + (5 \cdot 9) + (6 \cdot 11) = 28 + 45 + 66 = 139 & (4 \cdot 8) + (5 \cdot 10) + (6 \cdot 12) = 32 + 50 + 72 = 154
    \end{pmatrix}
    \]

    ---

    :hammer: ä¸‹é¢ï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸‰ä¸ªæƒé‡çŸ©é˜µï¼›è¯·æ³¨æ„ï¼Œä¸ºäº†ä¾¿äºè¯´æ˜ï¼Œæˆ‘ä»¬è®¾ç½®äº† `require_grad=False` ä»¥å‡å°‘è¾“å‡ºä¸­çš„æ··ä¹±ï¼Œä½†å¦‚æœæˆ‘ä»¬è¦ä½¿ç”¨æƒé‡çŸ©é˜µè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬å°†è®¾ç½® `require_grad=True` ä»¥åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æ›´æ–°è¿™äº›çŸ©é˜µã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(d_in, d_out):
    torch.manual_seed(123)

    W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
    W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
    W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

    mo.md(
        f"""
    + $W_{{q}}$
    ```python
    {W_query}
    ```

    + $W_{{k}}$
    ```python
    {W_key}
    ```

    + $W_{{v}}$
    ```python
    {W_value}
    ```
    """
    )
    return W_key, W_query, W_value


@app.cell(hide_code=True)
def _():
    mo.md(r""":hammer: æ¥ä¸‹æ¥æˆ‘ä»¬è®¡ç®—`Query`ã€`Key`å’Œ`Value`å‘é‡ï¼š""")
    return


@app.cell(hide_code=True)
def _(W_key, W_query, W_value, inputs, x_2):
    query_2 = (
        x_2 @ W_query
    )  # _2 because it's with respect to the 2nd input element
    keys = inputs @ W_key
    values = inputs @ W_value

    mo.md(
        f"""
    $X^{{(2)}}:$
    ```python
    {x_2}
    ```

    $Query^{{(2)}}:$
    ```python
    {query_2}
    ```

    $Keys:$
    ```python
    {keys}

    # Shape:
    {keys.shape}
    ```

    $Values:$
    ```python
    {values}

    #Shape:
    {values.shape}
    ```
    """
    )
    return keys, query_2, values


@app.cell
def _():
    0.5500 * 0.2961 + 0.8700 * 0.2517 + 0.6600 * 0.0740
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :hammer: é€šè¿‡è®¡ç®—`Query`å’Œæ¯ä¸ª`Key`å‘é‡ä¹‹é—´çš„ç‚¹ç§¯æ¥è®¡ç®—éæ ‡å‡†åŒ–ï¼ˆæœªå½’ä¸€åŒ–ï¼‰æ³¨æ„åŠ›åˆ†æ•°ï¼š

    ![2-14](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-14.svg)
    """
    )
    return


@app.cell(hide_code=True)
def _(keys, query_2):
    keys_2 = keys[1]  # Python starts index at 0
    attn_score_22 = query_2.dot(keys_2)

    mo.md(
        f"""
    Code:
    ```python
    keys_2 = keys[1]  # Python starts index at 0
    attn_score_22 = query_2.dot(keys_2)
    ```

    æ—¢ï¼š
    $W_{{k^{{(1)}}}} \\cdot Q^{{(2)}}$

    ---

    + $W_{{k^{{(1)}}}}$
    ```python
    {keys_2}
    ```

    + $Q^{{(2)}}$
    ```python
    {query_2}
    ```

    + Result:
    ```python
    {attn_score_22}
    ```
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":hammer: ç”±äºæˆ‘ä»¬æœ‰ 6 ä¸ªè¾“å…¥ï¼Œå› æ­¤å¯¹äºç»™å®šçš„æŸ¥è¯¢å‘é‡æˆ‘ä»¬æœ‰ 6 ä¸ªæ³¨æ„åŠ›åˆ†æ•°ï¼š"""
    )
    return


@app.cell(hide_code=True)
def _(keys, query_2):
    attn_scores_u2 = query_2 @ keys.T  # All attention scores for given query

    mo.md(
        f"""
    Code:
    ```python
    attn_scores_u2 = query_2 @ keys.T  # All attention scores for given query
    ```

    Result:
    ```python
    {attn_scores_u2}
    ```
    """
    )
    return (attn_scores_u2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :hammer: ç°åœ¨å¾—åˆ°çš„æ˜¯æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›æƒé‡ï¼Œæ¥ä¸‹æ¥éœ€è¦å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–æ“ä½œã€‚

    ![2-15](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-15.svg)
    """
    )
    return


@app.cell(hide_code=True)
def _(attn_scores_u2, keys):
    d_k = keys.shape[1]
    attn_weights_s2 = torch.softmax(attn_scores_u2 / d_k**0.5, dim=-1)


    mo.md(
        f"""
    Code:
    ```python
    d_k = keys.shape[1]
    attn_weights_s2 = torch.softmax(attn_scores_u2 / d_k**0.5, dim=-1)
    ```

    Result:
    ```python
    {attn_weights_s2}
    ```
    """
    )
    return (attn_weights_s2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :hammer: è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡ï¼š

    ![2-16](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-16.svg)
    """
    )
    return


@app.cell(hide_code=True)
def _(attn_weights_s2, values):
    context_vec_2 = attn_weights_s2 @ values

    mo.md(
        f"""
    $Attn$
    ```python
    {attn_weights_s2}
    ```

    $Values$
    ```python
    {values}
    ```

    $Context$
    ```python
    {context_vec_2}
    ```

    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(r"""### 2.4.2 å°è£…è‡ªæ³¨æ„åŠ›æœºåˆ¶ç±»""")
    return


@app.cell(hide_code=True)
def _(d_in, d_out, inputs):
    class SelfAttention_v1(nn.Module):

        def __init__(self, d_in, d_out):
            super().__init__()
            self.W_query = nn.Parameter(torch.rand(d_in, d_out))
            self.W_key = nn.Parameter(torch.rand(d_in, d_out))
            self.W_value = nn.Parameter(torch.rand(d_in, d_out))

        def forward(self, x):
            keys = x @ self.W_key
            queries = x @ self.W_query
            values = x @ self.W_value

            attn_scores = queries @ keys.T  # omega
            attn_weights = torch.softmax(
                attn_scores / keys.shape[-1] ** 0.5, dim=-1
            )

            context_vec = attn_weights @ values
            return context_vec


    torch.manual_seed(123)
    sa_v1 = SelfAttention_v1(d_in, d_out)
    print(sa_v1(inputs))
    mo.show_code()
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :rocket: æ•´ä½“è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

    ![2-17](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-17.svg)

    :hammer: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`PyTorch`çš„`Linear`å±‚ç®€åŒ–`SelfAttention`çš„ä»£ç ï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _(d_in, d_out, inputs):
    class SelfAttention_v2(nn.Module):

        def __init__(self, d_in, d_out, qkv_bias=False):
            super().__init__()
            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        def forward(self, x):
            keys = self.W_key(x)
            queries = self.W_query(x)
            values = self.W_value(x)

            attn_scores = queries @ keys.T
            attn_weights = torch.softmax(
                attn_scores / keys.shape[-1] ** 0.5, dim=-1
            )

            context_vec = attn_weights @ values
            return context_vec


    torch.manual_seed(789)
    sa_v2 = SelfAttention_v2(d_in, d_out)
    print(sa_v2(inputs))

    mo.show_code()
    return (sa_v2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :pushpin: è¯·æ³¨æ„ï¼Œ`SelfAttention_v1` å’Œ `SelfAttention_v2` ç»™å‡ºä¸åŒçš„è¾“å‡ºï¼Œå› ä¸ºå®ƒä»¬å¯¹æƒé‡çŸ©é˜µä½¿ç”¨ä¸åŒçš„åˆå§‹æƒé‡ã€‚

    ## 2.5 ç”¨å› æœæ³¨æ„åŠ›éšè—æœªæ¥çš„è¯è¯­

    > åœ¨å› æœæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå¯¹è§’çº¿ä¸Šæ–¹çš„æ³¨æ„åŠ›æƒé‡è¢«æ©ç›–ï¼Œç¡®ä¿å¯¹äºä»»ä½•ç»™å®šçš„è¾“å…¥ï¼ŒLLM åœ¨åˆ©ç”¨æ³¨æ„åŠ›æƒé‡è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡æ—¶æ— æ³•åˆ©ç”¨æœªæ¥çš„`Token`ã€‚

    ![2-18](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-18.svg)

    ### 2.5.1 åº”ç”¨å› æœæ³¨æ„åŠ›æ©ç 

    > å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿æ¨¡å‹å¯¹åºåˆ—ä¸­æŸä¸ªä½ç½®çš„é¢„æµ‹ä»…ä¾èµ–äºå…ˆå‰ä½ç½®çš„å·²çŸ¥è¾“å‡ºï¼Œè€Œä¸ä¾èµ–äºæœªæ¥ä½ç½®ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™ç¡®ä¿æ¯ä¸ªä¸‹ä¸€ä¸ªå•è¯çš„é¢„æµ‹ä»…ä¾èµ–äºå‰é¢çš„å•è¯ã€‚
    > 
    > ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå¯¹äºæ¯ä¸ªç»™å®šçš„`Token`ï¼Œæˆ‘ä»¬å±è”½æ‰æœªæ¥çš„`Token`ï¼ˆå³è¾“å…¥æ–‡æœ¬ä¸­å½“å‰`Token`ä¹‹åçš„`Token`ï¼‰ï¼š

    ![2-19](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-19.svg)

    :cloud: ä¸ºäº†è¯´æ˜å’Œå®ç°å› æœè‡ªæ³¨æ„åŠ›ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„æ³¨æ„åŠ›åˆ†æ•°å’Œæƒé‡ï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _(inputs, keys, sa_v2):
    queries_v2 = sa_v2.W_query(inputs)
    keys_v2 = sa_v2.W_key(inputs)
    attn_scores_v2 = queries_v2 @ keys_v2.T
    attn_weights_v2 = torch.softmax(attn_scores_v2 / keys.shape[-1] ** 0.5, dim=-1)

    print(attn_weights_v2)

    mo.show_code()
    return (attn_weights_v2,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":hammer: æ©ç›–æœªæ¥æ³¨æ„åŠ›æƒé‡çš„æœ€ç®€å•æ–¹æ³•æ˜¯é€šè¿‡ `PyTorch` çš„ `tril` å‡½æ•°åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œå°†ä¸»å¯¹è§’çº¿ä¸‹æ–¹çš„å…ƒç´ ï¼ˆåŒ…æ‹¬å¯¹è§’çº¿æœ¬èº«ï¼‰è®¾ç½®ä¸º `1`ï¼Œå°†ä¸»å¯¹è§’çº¿ä¸Šæ–¹çš„å…ƒç´ è®¾ç½®ä¸º `0`ï¼š"""
    )
    return


@app.cell(hide_code=True)
def _():
    get_tril_width, set_tril_width = mo.state(2)
    get_tril_height, set_tril_height = mo.state(2)
    import numpy as np


    def width_slider_on_change(value):
        set_tril_width(value)


    def height_slider_on_change(value):
        set_tril_height(value)


    width_slider = mo.ui.slider(
        start=2,
        stop=12,
        step=1,
        on_change=width_slider_on_change,
        label="Tensor Rows:",
    )
    height_slider = mo.ui.slider(
        start=2,
        stop=12,
        step=1,
        on_change=height_slider_on_change,
        label="Tensor Columns",
    )


    def generate_latex_tril_matrix(width, height):
        # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„çŸ©é˜µ
        matrix = np.zeros((height, width))

        # å¡«å……ä¸‹ä¸‰è§’éƒ¨åˆ†
        for i in range(min(height, width)):
            for j in range(i + 1):
                matrix[i][j] = 1

        # ç”Ÿæˆ LaTeX çŸ©é˜µæ ¼å¼
        latex_matrix = "$$\n\\left[\\begin{matrix}\n"
        for row in matrix:
            latex_matrix += " & ".join(map(str, row.astype(int))) + " \\\\\n"
        latex_matrix += "\\end{matrix}\\right]\n$$"

        return latex_matrix
    return (
        generate_latex_tril_matrix,
        get_tril_height,
        get_tril_width,
        height_slider,
        width_slider,
    )


@app.cell(hide_code=True)
def _(
    generate_latex_tril_matrix,
    get_tril_height,
    get_tril_width,
    height_slider,
    width_slider,
):
    mo.vstack(
        [
            mo.md("`torch.tril` Example:"),
            width_slider,
            height_slider,
            mo.md(generate_latex_tril_matrix(get_tril_width(), get_tril_height())),
        ]
    )
    return


@app.cell(hide_code=True)
def _(attn_scores):
    context_length = attn_scores.shape[0]
    mask_simple = torch.tril(torch.ones(context_length, context_length))
    print(mask_simple)

    mo.show_code()
    return context_length, mask_simple


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":hammer: å°† `mask_simple` ä¸ `attn_weights_v2` ç›¸ä¹˜å³å¯å¾—åˆ°æ©ç åçš„æ³¨æ„åŠ›æƒé‡ï¼š"""
    )
    return


@app.cell(hide_code=True)
def _(attn_weights_v2, mask_simple):
    masked_simple = attn_weights_v2 * mask_simple

    print(masked_simple)
    mo.show_code()
    return (masked_simple,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":fire: ç°åœ¨è™½ç„¶å¾—åˆ°äº†æˆ‘ä»¬æƒ³è¦çš„çŸ©é˜µï¼Œä½†æ˜¯`Softmax`çš„ç»“æœè¢«ç ´åäº†ï¼Œæ— æ³•æ»¡è¶³æ¯è¡Œçš„æ€»å’Œä¸º`1`ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹æ–¹å¼è§£å†³ï¼š"""
    )
    return


@app.cell(hide_code=True)
def _(masked_simple):
    row_sums = masked_simple.sum(dim=-1, keepdim=True)
    masked_simple_norm = masked_simple / row_sums

    print(masked_simple_norm)
    mo.show_code()
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :fire: è™½ç„¶æˆ‘ä»¬ç°åœ¨ä»æŠ€æœ¯ä¸Šå·²ç»å®Œæˆäº†å› æœæ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç ï¼Œä½†è®©æˆ‘ä»¬ç®€è¦åœ°çœ‹ä¸€ä¸‹å®ç°ä¸Šè¿°ç›¸åŒç›®æ ‡çš„æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚

    :hammer: æˆ‘ä»¬ä¸æ˜¯å°†å¯¹è§’çº¿ä¸Šæ–¹çš„æ³¨æ„åŠ›æƒé‡æ¸…é›¶å¹¶é‡æ–°è§„èŒƒåŒ–ç»“æœï¼Œè€Œæ˜¯å¯ä»¥åœ¨å¯¹è§’çº¿ä¸Šæ–¹æœªè§„èŒƒåŒ–çš„æ³¨æ„åŠ›åˆ†æ•°è¿›å…¥ `softmax` å‡½æ•°ä¹‹å‰ç”¨è´Ÿæ— ç©·å¤§è¿›è¡Œå±è”½ï¼š

    ![2-20](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-20.svg)
    """
    )
    return


@app.cell(hide_code=True)
def _(attn_scores, context_length):
    mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
    masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
    print(mask)
    print(masked)
    mo.show_code()
    return (masked,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    å› ä¸º `mask` æ˜¯åœ¨ `softmax` ä¹‹å‰è¢«**åŠ åˆ°æ³¨æ„åŠ›å¾—åˆ†ä¸Š**çš„ã€‚

    + 1ï¸âƒ£ softmax çš„æ•°å­¦æ€§è´¨ï¼š

    $$
    \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
    $$

    å¦‚æœæ©ç ä½ç½®å¡«çš„æ˜¯ **0**ï¼Œé‚£ä¹ˆï¼š

    * è¢« `mask` çš„ `token` çš„åˆ†æ•°ä¸å—å½±å“ï¼›
    * `softmax` ä»ç„¶ä¼šåˆ†é…ä¸€éƒ¨åˆ†æ¦‚ç‡ç»™è¿™äº›ä½ç½®ï¼›
    * æ¨¡å‹ä»ç„¶å¯èƒ½â€œå·çœ‹â€æœªæ¥çš„ `token` â€”â€” å› æœæ€§è¢«ç ´åã€‚

    ---

    + 2ï¸âƒ£ å¦‚æœæ©ç ä½ç½®æ˜¯ **âˆ’âˆ**ï¼ˆæˆ–åœ¨æ•°å€¼å®ç°ä¸­ä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°ï¼Œå¦‚ âˆ’1e9ï¼‰ï¼š

    $$
    e^{-\infty} = 0
    $$

    åˆ™ `softmax` è¾“å‡ºä¸­å¯¹åº”ä½ç½®çš„æ¦‚ç‡ä¸¥æ ¼ä¸º 0ï¼Œ
    å³è¯¥ä½ç½®è¢«å®Œå…¨å±è”½ï¼Œä¸ä¼šå¯¹æ³¨æ„åŠ›ç»“æœäº§ç”Ÿä»»ä½•å½±å“ã€‚

    ---

    åˆå§‹åŒ–æ³¨æ„åŠ›æ©ç çŸ©é˜µæ—¶ï¼Œå¯¹è§’çº¿ä¸Šæ–¹ï¼ˆæœªæ¥ token åŒºåŸŸï¼‰ä½¿ç”¨ **âˆ’âˆ** è€Œä¸æ˜¯ 0ï¼Œæ˜¯ä¸ºäº†åœ¨ `softmax` ä¸­å°†è¿™äº›ä½ç½®çš„æ³¨æ„åŠ›æ¦‚ç‡ä¸¥æ ¼å‹åˆ¶ä¸º 0ï¼Œä»è€Œç¡®ä¿æ¨¡å‹çš„å› æœæ€§ï¼ˆå³å½“å‰ `token` åªèƒ½çœ‹è§è¿‡å»å’Œè‡ªå·±ï¼Œä¸èƒ½çœ‹è§æœªæ¥ï¼‰ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(keys, masked):
    attn_weights_v3 = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)

    print(attn_weights_v3)
    mo.show_code()
    return (attn_weights_v3,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ### 2.5.2 ä½¿ç”¨Dropoutæ©ç›–æ³¨æ„åŠ›æƒé‡

    > æˆ‘ä»¬ç»å¸¸ä½¿ç”¨`Dropout`çš„æ–¹å¼æ¥é¿å…è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡åº¦æ‹Ÿåˆï¼Œå…¶æ ¸å¿ƒåŸç†å°±æ˜¯åœ¨è®­ç»ƒæ—¶ä¼šéšæœºå±è”½æ‰ä¸€éƒ¨åˆ†æ³¨æ„åŠ›æƒé‡ï¼Œä»¥é˜²æ­¢è®­ç»ƒæ—¶è¿‡åº¦ä¾èµ–æŸä¸ªå‚æ•°ã€‚

    ![2-21](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-21.svg)

    :rocket: å¦‚æœæˆ‘ä»¬åº”ç”¨ `0.5ï¼ˆ50%` çš„ `dropout` ç‡ï¼Œåˆ™æœªä¸¢å¼ƒçš„å€¼å°†ç›¸åº”ç¼©æ”¾ $\frac{1}{0.5} = 2$ã€‚

    è¿™æ˜¯ä¸€ä¸ªéå¸¸æ ¸å¿ƒä½†å®¹æ˜“è¢«å¿½ç•¥çš„ç»†èŠ‚ï¼Œä¸‹é¢æˆ‘ç»™å‡ºæ­£å¼ã€ç³»ç»Ÿçš„è§£é‡Šã€‚

    ---

    /// admonition | Dropout çš„æœ¬è´¨

    Dropout çš„ç›®çš„æ˜¯åœ¨è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒï¼ˆç½®é›¶ï¼‰éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    è®¾ä¸€ä¸ªç¥ç»å…ƒè¾“å‡ºä¸º$x$ï¼Œ`Dropout`ä»¥æ¦‚ç‡ $p$ ä¸¢å¼ƒè¯¥ç¥ç»å…ƒï¼ˆå³ä»¤è¾“å‡ºä¸º 0ï¼‰ï¼Œä»¥æ¦‚ç‡ $1-p$ ä¿ç•™ã€‚

    äºæ˜¯æˆ‘ä»¬å®šä¹‰æ©ç ï¼ˆmaskï¼‰ï¼š

    $$
    m_i \sim \text{Bernoulli}(1-p)
    $$

    > â€œéšæœºå˜é‡ $m_i$ æœä»å‚æ•°ä¸º $1-p$ çš„ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆBernoulli distributionï¼‰ã€‚â€

    Dropout åçš„è¾“å‡ºï¼š

    $$
    y_i = m_i \cdot x_i
    $$

    ---

    æ²¡æœ‰è¢«å‰”é™¤çš„å€¼ä¼šç¿»å€çš„å…³é”®åœ¨äº **ä¿æŒæœŸæœ›ä¸€è‡´ï¼ˆexpected value consistencyï¼‰**ã€‚

    > æˆ‘ä»¬å¸Œæœ›åœ¨ **è®­ç»ƒæ—¶ä½¿ç”¨ Dropout** å’Œ **æ¨ç†æ—¶ä¸ä½¿ç”¨ Dropout**ï¼Œä¸¤è€…çš„è¾“å‡ºæœŸæœ›ä¸€è‡´ï¼Œå¦åˆ™ç½‘ç»œåœ¨æ¨ç†é˜¶æ®µçš„æ¿€æ´»åˆ†å¸ƒä¼šå’Œè®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

    è®¾ç¥ç»å…ƒåŸå§‹è¾“å‡ºä¸º$x$ï¼Œä¸¢å¼ƒæ¦‚ç‡ä¸º$p$ã€‚

    * **è®­ç»ƒæ—¶ï¼ˆä½¿ç”¨ Dropoutï¼‰**

    $$
    E[y] = E[m \cdot x] = (1-p) \cdot x
    $$

    * **æ¨ç†æ—¶ï¼ˆä¸ä½¿ç”¨ Dropoutï¼‰**

    $$
    y_{\text{infer}} = x
    $$

    å¯ä»¥çœ‹åˆ°æœŸæœ›å€¼å˜äº†ï¼Œä» $(1-p)x$ å˜æˆ $p$ã€‚

    ä¸ºä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæ—¶å°†æœªä¸¢å¼ƒçš„ç¥ç»å…ƒè¾“å‡ºé™¤ä»¥ $1-p$ï¼Œå³ï¼š

    $$
    y = \frac{m \cdot x}{1-p}
    $$

    æ­¤æ—¶ï¼š

    $$
    E[y] = (1-p) \cdot \frac{x}{1-p} = x
    $$

    è¿™æ ·è®­ç»ƒå’Œæ¨ç†çš„è¾“å‡ºåˆ†å¸ƒå°±ä¸€è‡´äº†ã€‚

    ---

    :fire: Dropout æ—¶æœªè¢«ä¸¢å¼ƒçš„å€¼å˜å¤§ï¼Œæ˜¯å› ä¸ºåœ¨è®­ç»ƒé˜¶æ®µä¸ºäº†ä¿æŒè¾“å‡ºçš„**æœŸæœ›ä¸æ¨ç†é˜¶æ®µä¸€è‡´**ï¼Œæ¡†æ¶é‡‡ç”¨äº†â€œåå‘ Dropoutâ€æœºåˆ¶ï¼Œå°†æœªä¸¢å¼ƒçš„æ¿€æ´»é™¤ä»¥ä¿ç•™æ¦‚ç‡$(1-p)$ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(attn_weights_v3):
    torch.manual_seed(123)
    dropout = torch.nn.Dropout(0.5)  # dropout rate of 50%
    example = torch.ones(6, 6)  # create a matrix of ones

    print(dropout(example))
    print(dropout(attn_weights_v3))

    mo.show_code()
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ### 2.5.3 å®ç°å› æœè‡ªæ³¨æ„åŠ›

    > æˆ‘ä»¬å°†ä¸Šè¿°è¿‡ç¨‹å°è£…ä¸ºä¸€ä¸ªå› æœæ³¨æ„åŠ›æœºåˆ¶çš„å®ç°ç±» `CausalAttention`ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(inputs):
    batch = torch.stack((inputs, inputs), dim=0)
    print(inputs.shape)
    print(
        batch.shape
    )  # 2 inputs with 6 tokens each, and each token has embedding dimension 3

    mo.show_code()
    return (batch,)


@app.cell(hide_code=True)
def _(batch, d_in, d_out):
    class CausalAttention(nn.Module):

        def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
            super().__init__()
            self.d_out = d_out
            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.dropout = nn.Dropout(dropout)  # New
            self.register_buffer(
                "mask",
                torch.triu(torch.ones(context_length, context_length), diagonal=1),
            )  # New

        def forward(self, x):
            b, num_tokens, d_in = x.shape  # New batch dimension b
            # For inputs where `num_tokens` exceeds `context_length`, this will result in errors
            # in the mask creation further below.
            # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs
            # do not exceed `context_length` before reaching this forward method.
            keys = self.W_key(x)
            queries = self.W_query(x)
            values = self.W_value(x)

            attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose
            attn_scores.masked_fill_(
                # New, _ ops are in-place
                self.mask.bool()[:num_tokens, :num_tokens],
                -torch.inf,
            )
            # `:num_tokens` to account for cases
            # where the number of tokens in the batch is smaller
            # than the supported context_size
            attn_weights = torch.softmax(
                attn_scores / keys.shape[-1] ** 0.5, dim=-1
            )
            attn_weights = self.dropout(attn_weights)  # New

            context_vec = attn_weights @ values
            return context_vec


    torch.manual_seed(123)

    context_length_v2 = batch.shape[1]
    ca = CausalAttention(d_in, d_out, context_length_v2, 0.0)

    context_vecs = ca(batch)

    print(context_vecs)
    print(context_vecs.shape)

    mo.show_code()
    return (CausalAttention,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :fire: è¯·æ³¨æ„ï¼Œdropout ä»…åœ¨è®­ç»ƒæœŸé—´åº”ç”¨ï¼Œè€Œä¸æ˜¯åœ¨æ¨ç†æœŸé—´åº”ç”¨

    ![2-22](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-22.svg)
    """
    )
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    ## 2.6 å°†å•å¤´æ³¨æ„åŠ›æ‰©å±•åˆ°å¤šå¤´æ³¨æ„åŠ›

    ### 2.6.1 å †å å¤šä¸ªå•å¤´æ³¨æ„åŠ›å±‚

    > ä»¥ä¸‹æ˜¯ä¹‹å‰å®ç°çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ€»ç»“ï¼ˆä¸ºäº†ç®€å•èµ·è§ï¼Œæœªæ˜¾ç¤ºå› æœå’Œ `Dropout Mask` ï¼‰ï¼Œè¿™ä¹Ÿç§°ä¸ºå•å¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š

    ![2-24](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-24.svg)

    :cloud: æˆ‘ä»¬åªéœ€å †å å¤šä¸ªå•å¤´æ³¨æ„åŠ›æ¨¡å—å³å¯è·å¾—å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼š

    ![2-25](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-25.svg)

    :fire: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨ä¸åŒçš„ã€å·²å­¦ä¹ çš„çº¿æ€§æŠ•å½±å¤šæ¬¡ï¼ˆå¹¶è¡Œï¼‰è¿è¡Œæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè”åˆå…³æ³¨æ¥è‡ªä¸åŒä½ç½®çš„ä¸åŒè¡¨å¾å­ç©ºé—´çš„ä¿¡æ¯ã€‚
    """
    )
    return


@app.cell(hide_code=True)
def _(CausalAttention, batch):
    class MultiHeadAttentionWrapper(nn.Module):

        def __init__(
            self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False
        ):
            super().__init__()
            self.heads = nn.ModuleList(
                [
                    CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)
                    for _ in range(num_heads)
                ]
            )

        def forward(self, x):
            return torch.cat([head(x) for head in self.heads], dim=-1)


    torch.manual_seed(123)

    context_length_v3 = batch.shape[1]  # This is the number of tokens
    _tmp_d_in, _tmp_d_out = 3, 2
    mha = MultiHeadAttentionWrapper(
        _tmp_d_in, _tmp_d_out, context_length_v3, 0.0, num_heads=2
    )

    context_vecs_v2 = mha(batch)

    print("Context Vectors:", context_vecs_v2)
    print("Shape:", context_vecs_v2.shape)
    mo.show_code()
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :cloud: åœ¨ä¸Šé¢çš„å®ç°ä¸­ï¼ŒåµŒå…¥ç»´åº¦ä¸º $4$ï¼Œå› ä¸ºæˆ‘ä»¬å°† $d_{out}=2$ ä½œä¸ºé”®ã€æŸ¥è¯¢ã€å€¼å‘é‡ä»¥åŠä¸Šä¸‹æ–‡å‘é‡çš„åµŒå…¥ç»´åº¦ã€‚ç”±äºæˆ‘ä»¬æœ‰ä¸¤ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå› æ­¤è¾“å‡ºåµŒå…¥ç»´åº¦ä¸º $2 \times {2} = 4$

    ### 2.6.2 é€šè¿‡æƒé‡åˆ†å‰²å®ç°å¤šå¤´æ³¨æ„åŠ›

    > è™½ç„¶ä¸Šé¢æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„ç›´è§‚ä¸”åŠŸèƒ½é½å…¨çš„å®ç°ï¼ˆåŒ…è£…äº†ä¹‹å‰çš„å•å¤´æ³¨æ„åŠ› `CausalAttention` å®ç°ï¼‰ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªåä¸º `MultiHeadAttention` çš„ç‹¬ç«‹ç±»æ¥å®ç°ç›¸åŒçš„åŠŸèƒ½ã€‚

    :rocket: æˆ‘ä»¬ä¸ä¼šä¸ºè¿™ä¸ªç‹¬ç«‹çš„ `MultiHeadAttention` ç±»è¿æ¥å•ä¸ªæ³¨æ„åŠ›å¤´ï¼Œè€Œæ˜¯åˆ›å»ºå•ä¸ª $W_{query}$ã€$W_{key}$ å’Œ $W_{value}$ æƒé‡çŸ©é˜µï¼Œç„¶åå°†å®ƒä»¬æ‹†åˆ†ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„å•ç‹¬çŸ©é˜µï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _(batch):
    class MultiHeadAttention(nn.Module):
        def __init__(
            self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False
        ):
            super().__init__()
            assert d_out % num_heads == 0, "d_out must be divisible by num_heads"

            self.d_out = d_out
            self.num_heads = num_heads
            self.head_dim = (
                d_out // num_heads
            )  # Reduce the projection dim to match desired output dim

            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
            self.out_proj = nn.Linear(
                d_out, d_out
            )  # Linear layer to combine head outputs
            self.dropout = nn.Dropout(dropout)
            self.register_buffer(
                "mask",
                torch.triu(torch.ones(context_length, context_length), diagonal=1),
            )

        def forward(self, x):
            b, num_tokens, d_in = x.shape
            # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,
            # this will result in errors in the mask creation further below.
            # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs
            # do not exceed `context_length` before reaching this forward method.

            keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
            queries = self.W_query(x)
            values = self.W_value(x)

            print(keys.shape, queries.shape, values.shape)
            # We implicitly split the matrix by adding a `num_heads` dimension
            # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
            values = values.view(b, num_tokens, self.num_heads, self.head_dim)
            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

            print(keys.shape, queries.shape, values.shape)

            # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
            keys = keys.transpose(1, 2)
            queries = queries.transpose(1, 2)
            values = values.transpose(1, 2)

            # Compute scaled dot-product attention (aka self-attention) with a causal mask
            attn_scores = queries @ keys.transpose(
                2, 3
            )  # Dot product for each head

            # Original mask truncated to the number of tokens and converted to boolean
            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

            # Use the mask to fill attention scores
            attn_scores.masked_fill_(mask_bool, -torch.inf)

            attn_weights = torch.softmax(
                attn_scores / keys.shape[-1] ** 0.5, dim=-1
            )
            attn_weights = self.dropout(attn_weights)

            # Shape: (b, num_tokens, num_heads, head_dim)
            context_vec = (attn_weights @ values).transpose(1, 2)

            # Combine heads, where self.d_out = self.num_heads * self.head_dim
            context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
            context_vec = self.out_proj(context_vec)  # optional projection

            return context_vec


    torch.manual_seed(123)
    batch_size, context_length_v4, d_in_v2 = batch.shape
    d_out_v2 = 2
    mha_v2 = MultiHeadAttention(
        d_in_v2, d_out_v2, context_length_v4, 0.0, num_heads=2
    )
    context_vecs_v3 = mha_v2(batch)

    print("Context Vectors:", context_vecs_v3)
    print("Shape:", context_vecs_v3.shape)

    mo.show_code()
    return


@app.cell(hide_code=True)
def _():
    mo.md(
        r"""
    :fire: è¯·æ³¨æ„ï¼Œä¸Šé¢æœ¬è´¨ä¸Šæ˜¯ `MultiHeadAttentionWrapper` çš„é‡å†™ç‰ˆæœ¬ï¼Œæ•ˆç‡æ›´é«˜ã€‚ç”±äºéšæœºæƒé‡åˆå§‹åŒ–ä¸åŒï¼Œç»“æœè¾“å‡ºçœ‹èµ·æ¥æœ‰ç‚¹ä¸åŒï¼Œä½†ä¸¤è€…éƒ½æ˜¯åŠŸèƒ½é½å…¨çš„å®ç°ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­å®ç°çš„ `GPT` ç±»ä¸­ä½¿ç”¨ã€‚

    /// admonition | å…³äºè¾“å‡ºç»´åº¦çš„è¯´æ˜

    + åœ¨ä¸Šé¢çš„ $MultiHeadAttention$ ä¸­ï¼Œæˆ‘ä½¿ç”¨äº† $d_{out}=2$ æ¥ä½¿ç”¨ä¸ä¹‹å‰çš„ $MultiHeadAttentionWrapper$ ç±»ç›¸åŒçš„è®¾ç½®

    + ç”±äºè¿æ¥ï¼Œ$MultiHeadAttentionWrapper$ è¿”å›è¾“å‡ºå¤´éƒ¨ç»´åº¦ $d_{out} \times num_{heads}$ï¼ˆå³ $2 \times {2} = 4$ï¼‰

    + ä½†æ˜¯ï¼Œ$MultiHeadAttention$ ç±»ï¼ˆä¸ºäº†ä½¿å…¶æ›´åŠ ç”¨æˆ·å‹å¥½ï¼‰å…è®¸æˆ‘ä»¬ç›´æ¥é€šè¿‡ $d_{out}$ æ§åˆ¶è¾“å‡ºå¤´éƒ¨ç»´åº¦ï¼›è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½® $d_{out} = 2$ï¼Œåˆ™è¾“å‡ºå¤´éƒ¨ç»´åº¦å°†ä¸º $2$ï¼Œæ— è®ºå¤´éƒ¨æ•°é‡æ˜¯å¤šå°‘

    + äº‹åçœ‹æ¥ï¼Œæ­£å¦‚è¯»è€…æŒ‡å‡ºçš„é‚£æ ·ï¼Œä½¿ç”¨ $d_{out} = 4$ çš„ $MultiHeadAttention$ å¯èƒ½æ›´ç›´è§‚ï¼Œè¿™æ ·å®ƒäº§ç”Ÿçš„è¾“å‡ºç»´åº¦ä¸ $d_{out} = 2$ çš„ $MultiHeadAttentionWrapper$ ç›¸åŒ

    ///

    /// admonition | å…¶ä»–è¯´æ˜

    éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢ `MultiHeadAttention` ç±»ä¸­æ·»åŠ äº†ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚ (`self.out_proj`)ã€‚è¿™åªæ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œä¸ä¼šæ”¹å˜ç»´åº¦ã€‚åœ¨ `LLM` å®ç°ä¸­ä½¿ç”¨è¿™æ ·çš„æŠ•å½±å±‚æ˜¯æ ‡å‡†æƒ¯ä¾‹ï¼Œä½†å¹¶éç»å¯¹å¿…è¦ï¼ˆæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥å°†å…¶ç§»é™¤è€Œä¸ä¼šå½±å“å»ºæ¨¡æ€§èƒ½ï¼›è¯·å‚é˜…æœ¬ç« æœ«å°¾çš„å»¶ä¼¸é˜…è¯»éƒ¨åˆ†ï¼‰ã€‚

    ///

    ![2-23](https://codingsoul-images.tos-cn-beijing.volces.com/LLM/2-23.svg)

    :rocket: ç”±äºä¸Šè¿°å®ç°ä¹ä¸€çœ‹å¯èƒ½æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ‰§è¡Œ `attn_scores = queries @ keys.transpose(2, 3)` æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼š
    """
    )
    return


@app.cell(hide_code=True)
def _():
    # (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)
    a = torch.tensor(
        [
            [
                [
                    [0.2745, 0.6584, 0.2775, 0.8573],
                    [0.8993, 0.0390, 0.9268, 0.7388],
                    [0.7179, 0.7058, 0.9156, 0.4340],
                ],
                [
                    [0.0772, 0.3565, 0.1479, 0.5331],
                    [0.4066, 0.2318, 0.4545, 0.9737],
                    [0.4606, 0.5159, 0.4220, 0.5786],
                ],
            ]
        ]
    )

    mo.md(
        f"""

    $a$ ({a.shape}): 

    ```python
    {a}
    ```

    > `a.transpose(2, 3)`

    $a^T$ ({a.transpose(2, 3).shape}): 

    ```python
    {a.transpose(2, 3)}
    ```

    $a \\times a^T$ ({(a @ a.transpose(2, 3)).shape}): 

    ```python
    {a @ a.transpose(2, 3)}
    ```
    """
    )
    return (a,)


@app.cell(hide_code=True)
def _():
    mo.md(
        r""":hammer: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`PyTorch` ä¸­çš„çŸ©é˜µä¹˜æ³•å®ç°å°†å¤„ç† `4` ç»´è¾“å…¥å¼ é‡ï¼Œä»¥ä¾¿åœ¨æœ€å `2` ä¸ªç»´åº¦ï¼ˆ`num_tokens`ã€`head_dim`ï¼‰ä¹‹é—´è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œç„¶åå¯¹å„ä¸ªå¤´éƒ¨é‡å¤æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æˆä¸ºä¸€ç§æ›´ç´§å‡‘çš„æ–¹å¼æ¥åˆ†åˆ«è®¡ç®—æ¯ä¸ªå¤´éƒ¨çš„çŸ©é˜µä¹˜æ³•ï¼š"""
    )
    return


@app.cell(hide_code=True)
def _(a):
    first_head = a[0, 0, :, :]
    first_res = first_head @ first_head.T

    second_head = a[0, 1, :, :]
    second_res = second_head @ second_head.T

    mo.md(
        f"""
    ç¬¬ä¸€ä¸ªå¤´:
    ```python
    {first_res}
    ```

    ç¬¬äºŒä¸ªå¤´:
    ```python
    {second_res}
    ```
    """
    )
    return


if __name__ == "__main__":
    app.run()
